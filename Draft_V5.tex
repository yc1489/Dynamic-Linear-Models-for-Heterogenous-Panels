<<<<<<< HEAD
\documentclass[12pt,a4paper,hyperref]{article}
\usepackage[usenames,dvipsnames]{xcolor}
\definecolor{darkblue}{rgb}{0.0, 0.0, 0.55}
	\definecolor{ultramarine}{rgb}{0.07, 0.04, 0.56}
\usepackage{amsmath, natbib, latexsym, array, amssymb,longtable,float, graphicx, appendix,lscape,diagbox,textcomp,placeins}
\usepackage[colorlinks,
            linkcolor=ultramarine,
            anchorcolor=green,
            citecolor=darkblue
            ]{hyperref}
\usepackage[flushleft]{threeparttable}
\usepackage[top=2.7cm, left=3cm, right=3cm, bottom=2.7cm]{geometry}
\usepackage{hyperref}
\newtheorem{myDef}{Definition}
\newtheorem{myTheo}{Theorem}
\newtheorem{myProp}{proposition}
\newtheorem{myRem}{Remark}
\newtheorem{myAssu}{Assumption}
\newtheorem{myCor}{Corollary}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}

\urlstyle{same}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{pgfplotstable}
\sisetup{
  round-mode          = places, % Rounds numbers
  round-precision     = 2, % to 2 places
}

\newenvironment{sequation}{\begin{equation}\tiny}{\end{equation}}
\DeclareMathOperator*{\plim}{plim}
\renewcommand{\floatpagefraction}{0.60}
\renewcommand{\appendixpagename}{\Large Appendix}
\setcounter{secnumdepth}{3}
\begin{document}
\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page



\HRule \\[0.4cm]
{ \huge \bfseries Dynamic Heterogeneous Panels }\\[0.4cm] % Title of your document
\HRule \\[1.5cm]


\vfill % Fill the rest of the page with whitespace

\end{titlepage}

\newpage
\tableofcontents
\newpage
\section{Model, asymptotic property of LS and IV estimator}
\subsection{The models and Assumptions}
\subsubsection{Fixed effects model}
Consider the dynamic heterogeneous panels data model with fixed effects:
\begin{align}
\begin{split}
y_{i,t}&=\alpha_{i}+\phi_{i} y_{i,t-1}+ \boldsymbol{x}^{'}_{i,t}\boldsymbol{\beta}_{i}+u_{i,t}, \,\, for\,\,i=1,\ldots N;\,t=1,\ldots,T\, , \\
&= \boldsymbol{w}^{'}_{i,t}\boldsymbol{\theta}+ \alpha_{i}+u_{i,t}, \label{1}
\end{split}
\end{align}
where $\boldsymbol{x}_{i,t}$ and $\boldsymbol{\beta}_{i}$ are $k \times 1$ vectors and $\boldsymbol{w}_{i,t}=\left(\phi_{i}, \boldsymbol{\beta}^{'}_{i} \right)^{'}$ is a $\left(1+k\right) \times 1$ vector.
Stacking the $T$ observations for each $i$, we have
\begin{align}
\boldsymbol{y}_{i}=\boldsymbol{W}_{i}\boldsymbol{\theta}_{i}+ \alpha_{i}\boldsymbol{\iota}_{T}+ \boldsymbol{u}_{i}, \label{2}
\end{align}
where $\boldsymbol{y}_{i}=\left(y_{i,1},\ldots, y_{i,T} \right)^{'}$, $\boldsymbol{W}_{i}=\left(\boldsymbol{w}_{i,1},\ldots, \boldsymbol{w}_{i,T} \right)^{'}$, $\iota_{T}=\left(1,\ldots,1 \right)^{'}$ and $\boldsymbol{u}_{i}=\left(u_{i,1}, \ldots, u_{i,T} \right)^{'}$. 

Due to the incidental parameters problem arise, we use forward filter to the model by \citet{Moon:2000}, \citet{Hayakawa:2009} and \citet{Hayakawa:2019}. We define the $\left(T-1 \right) \times 1$ forward demeaning matrix as

\begin{align}
\boldsymbol{F}=diag(c_{1}, c_{2}, \ldots c_{T-1})
\begin{bmatrix}
1 & \frac{-1}{T-1} & \cdots & \cdots & \frac{-1}{T-1}\\
\vdots & 1 & \frac{-1}{T-2} & \cdots & \frac{-1}{T-2}\\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & -1 
\end{bmatrix},
\end{align}    
where $c_{t}=\sqrt{\left( T-t\right)\left( T-t+1\right) }$.
Multiply $\boldsymbol{F}$ to model $(\ref{2})$, the model can expressed as  
\begin{align}
\tilde{\boldsymbol{y}}_{i}= \tilde{\boldsymbol{W}}_{i}\boldsymbol{\theta}+\tilde{\boldsymbol{u}}_{i},
\end{align}
where $\tilde{\boldsymbol{y}}_{i}=\boldsymbol{F}\boldsymbol{y}_{i}= \left(\tilde{\boldsymbol{y}}_{i,1}, \ldots, \tilde{\boldsymbol{y}}_{i,T-1} \right)^{'}$,  $\tilde{\boldsymbol{W}}_{i}=\boldsymbol{F}\boldsymbol{W}_{i}= \left(\tilde{\boldsymbol{w}}_{i,1}, \ldots, \tilde{\boldsymbol{w}}_{i,T-1} \right)^{'}$ and $\tilde{\boldsymbol{u}}_{i}=\boldsymbol{F}\boldsymbol{u}_{i}= \left(\tilde{\boldsymbol{u}}_{i,1}, \ldots, \tilde{\boldsymbol{u}}_{i,T-1} \right)^{'}$ with $\tilde{\boldsymbol{y}}_{i,t}=c_{t}\left[y_{i,t}-\left(y_{i,t+1}+\cdots+y_{i,T} \right)/\left(T-t \right) \right]$, for $t=1, \ldots, T-1.$


\subsubsection{Trend model}
Consider the dynamic heterogeneous panels data model with fixed effects and heterogeneous time trends:
\begin{align}
\begin{split}
y_{i,t}&=\alpha_{i}+\eta_{i}t +\phi_{i} y_{i,t-1}+ \boldsymbol{x}^{'}_{i,t}\boldsymbol{\beta}_{i}+u_{i,t}, \,\, for\,\,i=1,\ldots N;\,t=1,\ldots,T\, , \\
&= \boldsymbol{w}^{'}_{i,t}\boldsymbol{\theta}+ \alpha_{i}+\eta_{i}t+u_{i,t}, 
\end{split}
\end{align}
where $\boldsymbol{x}_{i,t}$ and $\boldsymbol{\beta}_{i}$ are $k \times 1$ vectors and $\boldsymbol{w}_{i,t}=\left(\phi_{i}, \boldsymbol{\beta}^{'}_{i} \right)^{'}$ is a $\left(1+k\right) \times 1$ vector.
Stacking the $T$ observations for each $i$, we have
\begin{align}
\boldsymbol{y}_{i}=\boldsymbol{W}_{i}\boldsymbol{\theta}_{i}+ \alpha_{i}\boldsymbol{\iota}_{T}+\eta_{i}\boldsymbol{\tau}_{T} + \boldsymbol{u}_{i}, \label{6}
\end{align}
where $\boldsymbol{\tau}_{T}=\left(1, 2, \ldots, T \right)^{'}$. Again, we define the forward demeaning matrix as

\begin{align}
\boldsymbol{F}^{\tau}=diag(c^{\tau}_{1}, c^{\tau}_{2}, \ldots c^{\tau}_{T-2})
\begin{bmatrix}
1 & \frac{2(-2(T-2))}{(T-1)(T-2)} & \frac{2(-2(T-2)+3)}{(T-1)(T-2)} & \cdots & \frac{2(-2(T-2)+3(T-2))}{(T-1)(T-2)} \\
0 & 1 & \frac{2(-2(T-3))}{(T-2)(T-3)} & \cdots & \frac{2(-2(T-3)+3(T-3))}{(T-3)(T-4)} \\
\vdots &\vdots & \vdots & \ddots  &\vdots \\
0 & \cdots & \cdots & \cdots & \frac{2(-2+3)}{2 \cdot 1}
\end{bmatrix}, \label {7}
\end{align}   
where $c^{\tau}_{t}=\sqrt((T-t)(T-t-1)/(T-t-1)(T-t+2)).$

Multiply $\boldsymbol{F}$ to model $(\ref{6})$, the model can expressed as  
\begin{align}
\tilde{\boldsymbol{y}}^{\tau}_{i}= \tilde{\boldsymbol{W}}^{\tau}_{i}\boldsymbol{\theta}+\tilde{\boldsymbol{u}}^{\tau}_{i},
\end{align}
where $\tilde{\boldsymbol{y}}^{\tau}_{i}=\boldsymbol{F}^{\tau}\boldsymbol{y}_{i}= \left(\tilde{\boldsymbol{y}}^{\tau}_{i,1}, \ldots, \tilde{\boldsymbol{y}}^{\tau}_{i,T-2} \right)^{'}$,  $\tilde{\boldsymbol{W}}^{\tau}_{i}=\boldsymbol{F}^{\tau}\boldsymbol{W}_{i}$ and $\tilde{\boldsymbol{u}}^{\tau}_{i}=\boldsymbol{F}^{\tau}\boldsymbol{u}_{i}.$







\begin{myAssu}
$x_{i,t}$ and $u_{i,t}$ are independently distributed for all $t$ and $s$ .
\end{myAssu}


\subsection{IV estimator}
We use current and lagged values of $\boldsymbol{x}_{i}$ as instruments, as
\begin{align}
\boldsymbol{Z}^{(j)}_{i}=\left( \boldsymbol{x}_{i,\cdot}, \boldsymbol{x}_{i,-1},\ldots, \boldsymbol{x}_{i,-j} \right)^{'},
\end{align}
where $\boldsymbol{Z}^{(j)}_{i}$ is $T \times  (j+1)k$ matrix with $j \geq 1$. \\




Then, we can define IV estimator, $\boldsymbol{\theta}^{b}_{i}$, as
\begin{align}
\hat{\boldsymbol{\theta}}^{b}_{IV,i}= \left( \tilde{\boldsymbol{W}}^{'}_{i}\boldsymbol{P}^{(j)}_{i} \tilde{\boldsymbol{W}}_{i} \right)^{-1} \tilde{\boldsymbol{W}}^{'}_{i}\boldsymbol{P}^{(j)}_{i} \tilde{\boldsymbol{y}}_{i},
\end{align}
where $\boldsymbol{P}^{(j)}_{i}=\boldsymbol{Z}^{(j)}_{i} \left(\boldsymbol{Z}^{(j)'}_{i}\boldsymbol{Z}^{(j)}_{i} \right)^{-1}\boldsymbol{Z}^{(j)'}_{i}$ and $b=1$ corresponds to the fixed effects model while $b=2$ corresponds to the trend models. 

If we can observe long lagged length from data, we have many instruments that can be used.
In empirical study, researchers do not use all past variables as instruments because there are trade off between efficiency and bias.
But we have not clearly know that the effect of using long lagged length  IV estimator in heterogeneous dynamic panel data model. And how to select the instruments to balance the bias and efficiency.

\citet{Kuersteiner:2010} provided model averaging two stage least squares estimator to balance the bias and efficiency. We try to follow this method to construct the optimal instruments. We define a weighting vector $\boldsymbol{W}=\left(w_{1}, \ldots, w_{j} \right)^{'}$. Then, we can weight $\boldsymbol{P}^{(j)}_{i}$ as
\begin{align}
\boldsymbol{P}\left(\boldsymbol{W} \right)= \sum^{J}_{j=1}w_{j} \boldsymbol{P}^{(j)}_{i},
\end{align}
where $J$ is maximum number of lagged variables that we can observed. 
Our goal is choosing $\boldsymbol{W}$ to minimize the approximate mean square error. 



\subsection{Asymptotic property of LS estimator}


Based on heterogenous dynamic panel data model $(\ref{1})$, we can obtain fixed effect estimator as
\begin{align}
\hat{\boldsymbol{\theta}}_{LS,i}=
\begin{pmatrix}
\hat{\phi}_{i} \\
\hat{\beta}_{i}
\end{pmatrix}=
\begin{pmatrix}
\frac{\sum^{T}_{t=1}\tilde{y}^{2}_{i,t-1}}{T} & \frac{\sum^{T}_{t=1}\tilde{y}_{i,t-1}\tilde{x}_{i,t}}{T} \\
\frac{\sum^{T}_{t=1}\tilde{x}_{i,t}\tilde{y}_{i,t-1}}{T} &  \frac{\sum^{T}_{t=1}\tilde{x}_{i,t}^{2}}{T}
\end{pmatrix}^{-1}
\begin{pmatrix}
\frac{\sum^{T}_{t=1}\tilde{y}_{i,t-1}\tilde{y}_{i,t}}{T} \\
\frac{\sum^{T}_{t=1}\tilde{x}_{i,t}\tilde{y}_{i,t}}{T}
\end{pmatrix},
\end{align}

where $\tilde{y}_{i,t}=y_{i,t}-\bar{y}_{i}$, $\tilde{y}_{i,t-1}=y_{i,t-1}-\bar{y}_{i,-1}\iota_{T}$ and $\tilde{x}_{i,t}=x_{i,t}-\bar{x}_{i}$ with $\bar{y}_{i}=\frac{1}{T}\sum^{T}_{t=1}y_{i,t}$, $\bar{y}_{i,-1}=\frac{1}{T}\sum^{T}_{t=1}y_{i,t-1}$,  $\bar{x}_{i}=\frac{1}{T}\sum^{T}_{t=1}x_{i,t}$.  Under equation $(\ref{1})$, we have
\begin{align}
\begin{pmatrix}
\hat{\phi}_{i}-\phi_{i} \\
\hat{\beta}_{i}-\beta_{i}
\end{pmatrix}=
\begin{pmatrix}
\frac{\sum^{T}_{t=1}\tilde{y}^{2}_{i,t-1}}{T} & \frac{\sum^{T}_{t=1}\tilde{y}_{i,t-1}\tilde{x}_{i,t}}{T} \\
\frac{\sum^{T}_{t=1}\tilde{x}_{i,t}\tilde{y}_{i,t-1}}{T} &  \frac{\sum^{T}_{t=1}\tilde{x}_{i,t}^{2}}{T}
\end{pmatrix}^{-1}
\begin{pmatrix}
\frac{\sum^{T}_{t=1}\tilde{y}_{i,t-1}\tilde{u}_{i,t}}{T} \\
\frac{\sum^{T}_{t=1}\tilde{x}_{i,t}\tilde{u}_{i,t}}{T}
\end{pmatrix},
\end{align}
where $\tilde{u}_{i,t}$ is $u_{i,t}-\bar{u}_{i}$ with $\bar{u}_{i}=\frac{1}{T}\sum^{T}_{t=1}u_{i,t}.$


Now, we can investigate asymptotic bias by taking the probability limit as
\begin{align}
A^{(1)}_{\phi i}=\plim_{T  \rightarrow \infty}\left( \frac{\sum^{T}_{t=1}\tilde{y}_{i,t-1}\tilde{u}_{i,t}}{T} \right). \label{11}
\end{align}
Then $A^{(1)}_{i}$ can be taken expectations as
\begin{align}
\begin{split}
A^{(1)}_{\phi i}&=E\left(y_{i,t-1}-\bar{y}_{i,-1} \right) \left(u_{i,t}-\bar{u}_{i} \right) \\
&= E\left( y_{i,t-1}u_{i,t}\right) - E\left( y_{i,t-1} \bar{u}_{i}  \right)-E\left(\bar{y}_{i,-1}   u_{i,t}\right)+E\left(\bar{y}_{i,-1}  \bar{u}_{i}\right),
\end{split}
\end{align}
where $E\left( y_{i,t-1}u_{i,t}\right)=0$.

And we assume $y_{i,t}$ has started from a long time period in the past, so we have
\begin{align}
y_{i,t}=\frac{\alpha_{i}}{\left(1-\phi_{i} \right)}+\sum^{\infty}_{s=0}\beta_{i}\phi^{s}_{i}x_{i,t -s}+\sum^{\infty}_{s=0}\phi^{s}_{i}u_{i,t-s},
\end{align}

Then, we have
\begin{align}
\begin{split}
A^{(1)}_{\phi i}&=-E\left( \left(\sum^{\infty}_{s=0}\phi^{s}_{i}u_{i,t-s-1} \right) \left(\frac{1}{T}\sum^{T}_{t=1}u_{i,t} \right)  \right)-E\left(\frac{u_{i,t}}{T}\sum^{T}_{t=1}\sum^{\infty}_{s=0}\phi^{s}_{i}u_{i,t-s-1} \right)+ \\
&\left(\frac{1}{T}\sum^{T}_{t=1}\sum^{\infty}_{s=0}\phi^{s}_{i}u_{i,t-s-1} \right)\left( \frac{1}{T}\sum^{T}_{t=1}u_{i,t} \right). \label{12}
\end{split}
\end{align}
 Hence, from above equation, we have
 \begin{equation}
\begin{split}
A^{(1)}_{\phi i}=&-\dfrac{1}{T} E\left\{  \left( u_{i,t-1}+u_{i,t-2}\phi_{i}^{1}+u_{i,t-3}\phi_{i}^{2}+\ldots\right)\left(u_{i,1}+\ldots+u_{i,t-1}+u_{i,t}+\ldots+u_{i,T}\right) \right\} -\\
 &\dfrac{1}{T} E \left\{  u_{i,t}\sum^{T}_{s=1}\left( u_{i,s-1}\phi_{i}^{0}+u_{i,s-2}\phi_{i}^{1}+\ldots+u_{i,s-t-1}\phi_{i}^{t}+\ldots+u_{i,s-T-1}\phi_{i}^{T}+\ldots \right)  \right\}  + \\
 +& \dfrac{1}{T} E\{ \left( \sum^{T}_{s=1}u_{i,s-1}\phi_{i}^{0}+\sum^{T}_{s=1}u_{i,s-2}\phi_{i}^{1}+\ldots+\sum^{T}_{s=1}u_{i,s-t-1}\phi_{i}^{t}+\ldots
+\sum^{T}_{s=1}u_{is-T-1}\phi_{i}^{T}+\ldots \right)\\
 &\left(\dfrac{1}{T}\sum^{T}_{s=1}u_{i,s}\right) \} \\
 =&-\dfrac{\sigma_{u}^{2}}{T}\dfrac{(1-\phi_{i}^{t-1})}{1-\phi_{i}}-\dfrac{\sigma^{2}_{u}}{T}\dfrac{(1-\phi_{i}^{T-t})}{(1-\phi_{i})}+\dfrac{\sigma^{2}_{u}}{T} \left( \dfrac{1}{1-\phi_{i}}-\dfrac{1}{T}\dfrac{(1-\phi_{i}^{T})}{(1-\phi_{i})^{2}}\right) \\
 =&-\dfrac{\sigma_{u}^{2}}{T(1-\phi_{i})}\left( 1-\phi_{i}^{t-1}-\phi_{i}^{T-t}+\dfrac{1}{T}\dfrac{(1-\phi_{i}^{T})}{(1-\phi_{i})}  \right).
 \end{split}
\end{equation}


Therefore, we can see the bias of $\hat{\phi}_{i}$ is $O\left(T^{-1}\right)$.

To be more compact ,we can rewrite the model as,
\begin{align}
\tilde{\boldsymbol{y}}_{i}=\tilde{\boldsymbol{W}}_{i}\boldsymbol{\theta}_{i}+\tilde{\boldsymbol{u}}_{i},
\end{align}
where $\tilde{\boldsymbol{y}}_{i}=\left(\tilde{\boldsymbol{y}}_{i,1},\ldots, \tilde{\boldsymbol{y}}_{i,T}  \right)^{'}$ is $T \times 1$ vector,  $\tilde{\boldsymbol{W}}_{i}=\left( \tilde{\boldsymbol{w}}_{i,1},\ldots, \tilde{\boldsymbol{w}}_{i,T} \right)^{'}$ is $T \times 2$ matrix and $\tilde{\boldsymbol{u}}_{i}=\left(\tilde{\boldsymbol{u}}_{i,1}, \ldots, \tilde{\boldsymbol{u}}_{i,T} \right)$ is $T \times 1$ vector with $\tilde{\boldsymbol{w}}_{i,t}=\left(y_{i,t-1}-\bar{y}_{i,-1}, x_{i,t}-\bar{x}_{i}  \right)^{'}$, for $t=1,\ldots, T$ and $i=1,\ldots, N.$

Also, we define our interested parameter as
\begin{align}
\left(\phi_{i},\beta_{i}  \right)^{'}=\boldsymbol{\theta}_{i}=\boldsymbol{\theta}+\boldsymbol{\lambda}_{i},
\end{align}
where $\boldsymbol{\lambda}_{i}\overset{i.i.d.}{\sim} \left(\boldsymbol{0}, \boldsymbol{\Sigma}_{\lambda} \right).$
The lest square estimator, $\hat{\boldsymbol{\theta}}_{LS,i}$, can be expressed as
\begin{align}
\hat{\boldsymbol{\theta}}_{LS,i}=\left(\frac{\tilde{\boldsymbol{W}}^{'}_{i} \tilde{\boldsymbol{W}}_{i}}{T}  \right)^{-1} \frac{ \tilde{\boldsymbol{W}}^{'}_{i}\tilde{\boldsymbol{y}}_{i}}{T}. \label{13}
\end{align}

From above discussion and assumptions, we have following theorem \\
\begin{myTheo}
\begin{align}
\sqrt{T}\left(\hat{\boldsymbol{\theta}}_{IV,i}-\boldsymbol{\theta}_{i}\right) \overset{d}{\to} N \left(\boldsymbol{0}, \boldsymbol{Q}_{i}^{-1}\boldsymbol{\Sigma}_{LS,i}\boldsymbol{Q}_{i}^{-1}  \right),
\end{align}
where $\boldsymbol{\Sigma}_{LS,i}=\plim_{T \to \infty} T^{-1}\tilde{\boldsymbol{W}}^{'}_{i} \tilde{\boldsymbol{u}}_{i} \tilde{\boldsymbol{u}}^{'}_{i}  \tilde{\boldsymbol{W}}_{i} $ and $\boldsymbol{Q}_{i}=\plim_{T \to \infty} T^{-1}\tilde{\boldsymbol{W}}^{'}_{i}\tilde{\boldsymbol{W}}_{i}$
\end{myTheo}



\subsubsection{Mean group LS estimator }
Now, we define the mean group estimator of $\boldsymbol{\theta}$:
\begin{align}
\hat{\boldsymbol{\theta}}_{LSMG}=\frac{1}{N}\sum^{N}_{i=1}\hat{\boldsymbol{\theta}}_{LSi}.
\end{align}
And we can show that the asymptotic property of $\hat{\boldsymbol{\theta}}_{LSMG}$, as
\begin{align}
\begin{split}
\hat{\boldsymbol{\theta}}_{LSMG}&=N^{-1}\sum^{N}_{i=1}\left(\frac{\tilde{\boldsymbol{W}}^{'}_{i} \tilde{\boldsymbol{W}}_{i}}{T}  \right)^{-1} \frac{ \tilde{\boldsymbol{W}}^{'}_{i}\tilde{\boldsymbol{y}}_{i}}{T} \\
&=\bar{\boldsymbol{\theta}}+N^{-1}\sum^{N}_{i=1}\left(\frac{\tilde{\boldsymbol{W}}^{'}_{i} \tilde{\boldsymbol{W}}_{i}}{T}  \right)^{-1} \frac{ \tilde{\boldsymbol{W}}^{'}_{i}\tilde{\boldsymbol{u}}_{i}}{T},
\end{split}
\end{align}
where $\bar{\boldsymbol{\theta}}=N^{-1}\sum^{N}_{i=1}\boldsymbol{\theta}_{i}.$
For fixed $N$ and large $T$, we have
\begin{align}
\plim_{T \to \infty}\hat{\boldsymbol{\theta}}_{LSMG} =  \bar{\boldsymbol{\theta}}+ N^{-1}\sum^{N}_{i=1}\plim_{T \to \infty}\left(\frac{\tilde{\boldsymbol{W}}^{'}_{i} \tilde{\boldsymbol{W}}_{i}}{T}  \right)^{-1}\plim_{T \to \infty}\left( \frac{ \tilde{\boldsymbol{W}}^{'}_{i}\tilde{\boldsymbol{u}}_{i}}{T}\right)
\end{align}
 Then, from section $1.1$, we know that $\plim_{T \to \infty}\left( \frac{ \tilde{\boldsymbol{W}}^{'}_{i}\tilde{\boldsymbol{u}}_{i}}{T}\right)=O_{p}(1)$. Thus,  we can obtain
\begin{align}
\plim_{T \to \infty}\hat{\boldsymbol{\theta}}_{LSMG} =  \bar{\boldsymbol{\theta}}.
\end{align}
When $N \to \infty$ and $T \to \infty$ and by the law of large numbers, we can see that
\begin{align}
\plim_{T \to \infty, N \to \infty}\hat{\boldsymbol{\theta}}_{LSMG}=\boldsymbol{\theta}.
\end{align}

And the variance estimator of $\hat{\boldsymbol{\theta}}_{LSMG}$ is given by
\begin{align}
\hat{\boldsymbol{\Sigma}}_{LS,\lambda}=\frac{1}{N-1}\sum^{N}_{i=1}\left( \hat{\boldsymbol{\theta}}_{LS,i}- \hat{\boldsymbol{\theta}}_{LSMG}\right)\left( \hat{\boldsymbol{\theta}}_{LS,i}- \hat{\boldsymbol{\theta}}_{LSMG}\right)^{'}.
\end{align}

We show that $\hat{\boldsymbol{\Sigma}}_{LS,\lambda}$ is consistent when $N \to \infty$ and $T \to \infty.$
\begin{align}
\begin{split}
&\sum^{N}_{i=1}\left( \hat{\boldsymbol{\theta}}_{LS,i}- \hat{\boldsymbol{\theta}}_{LSMG}\right)\left( \hat{\boldsymbol{\theta}}_{LS,i}- \hat{\boldsymbol{\theta}}_{LSMG}\right)^{'}=\sum^{N}_{i=1}\left( \hat{\boldsymbol{\theta}}_{LS,i}- E\left(\hat{\boldsymbol{\theta}}_{LS, i}\right) \right)\left( \hat{\boldsymbol{\theta}}_{LS,i}- E\left(\hat{\boldsymbol{\theta}}_{LS, i}\right)\right)^{'}\\
&+\sum^{N}_{i=1}\left( \hat{\boldsymbol{\theta}}_{LSMG}- E\left(\hat{\boldsymbol{\theta}}_{LS, i}\right)\right) \left( \hat{\boldsymbol{\theta}}_{LSMG}- E\left(\hat{\boldsymbol{\theta}}_{LS, i}\right)\right)^{'}\\
&-\sum^{N}_{i=1}\left( \hat{\boldsymbol{\theta}}_{LSMG}- E\left(\hat{\boldsymbol{\theta}}_{LS, i}\right)\right) \left( \hat{\boldsymbol{\theta}}_{LS,i}- E\left(\hat{\boldsymbol{\theta}}_{LS, i}\right)\right)^{'} \\
&-\sum^{N}_{i=1}\left( \hat{\boldsymbol{\theta}}_{LS,i}- E\left(\hat{\boldsymbol{\theta}}_{LS, i}\right)\right) \left( \hat{\boldsymbol{\theta}}_{LSMG}- E\left(\hat{\boldsymbol{\theta}}_{LS, i}\right)\right)^{'}. \label{20}
\end{split}
\end{align}

Taking expectation on equation $(\ref{20})$, we have
\begin{align}
\begin{split}
&E\left( \sum^{N}_{i=1}\left( \hat{\boldsymbol{\theta}}_{LS,i}- \hat{\boldsymbol{\theta}}_{LSMG}\right)\left( \hat{\boldsymbol{\theta}}_{LS,i}- \hat{\boldsymbol{\theta}}_{LSMG}\right)^{'} \right)=
 \sum^{N}_{i=1}Var\left(\hat{\boldsymbol{\theta}}_{LS,i}\right)+
\sum^{N}_{i=1}\hat{\boldsymbol{\theta}}_{LSMG}\hat{\boldsymbol{\theta}}^{'}_{LSMG}+  \\
&\sum^{N}_{i=1}E\left(\hat{\boldsymbol{\theta}}_{LS,i}\right) E\left(\hat{\boldsymbol{\theta}}_{LS,i}\right)^{'}-
\sum^{N}_{i=1}\hat{\boldsymbol{\theta}}_{LSMG}\hat{\boldsymbol{\theta}}^{'}_{LS,i}+
\sum^{N}_{i=1}E\left(\hat{\boldsymbol{\theta}}_{LS,i}\right)\hat{\boldsymbol{\theta}}^{'}_{LS,i}-
\sum^{N}_{i=1}\hat{\boldsymbol{\theta}}_{LSMG}E\left(\hat{\boldsymbol{\theta}}_{LS,i}\right)^{'}- \\
&\sum^{N}_{i=1}\hat{\boldsymbol{\theta}}_{LS,i}\hat{\boldsymbol{\theta}}^{'}_{LSMG}+
\sum^{N}_{i=1}\hat{\boldsymbol{\theta}}_{LS,i}E\left(\hat{\boldsymbol{\theta}}_{LS,i}\right)^{'}-
\sum^{N}_{i=1}E\left(\hat{\boldsymbol{\theta}}_{LS,i}\right) E\left(\hat{\boldsymbol{\theta}}_{LS,i}\right)^{'} = \\
&\sum^{N}_{i=1}Var\left(\hat{\boldsymbol{\theta}}_{LS,i}\right)-N E \left(\hat{\boldsymbol{\theta}}_{LSMG}\hat{\boldsymbol{\theta}}^{'}_{LSMG} \right)+\sum^{N}_{i=1}E\left(\hat{\boldsymbol{\theta}}_{LS,i}\right) E\left(\hat{\boldsymbol{\theta}}_{LS,i}\right)^{'}
\end{split}
\end{align}
and
\begin{align}
E \left(\hat{\boldsymbol{\theta}}_{LSMG}\hat{\boldsymbol{\theta}}^{'}_{LSMG} \right)=\frac{1}{N^{2}} \sum^{N}_{i=1}\sum^{N}_{j=1}E \left(\hat{\boldsymbol{\theta}}_{LS,i}\hat{\boldsymbol{\theta}}^{'}_{LS,i} \right).
\end{align}

And, we also have
\begin{align}
E \left(\hat{\boldsymbol{\theta}}_{LSMG}\hat{\boldsymbol{\theta}}^{'}_{LSMG} \right)=\frac{1}{N^{2}}\left( \sum^{N}_{i=1}Var\left(\hat{\boldsymbol{\theta}}_{LS,i}\right)+ \sum^{N}_{i=1}\sum^{N}_{j=1}E \left(\hat{\boldsymbol{\theta}}_{LS,i}\hat{\boldsymbol{\theta}}^{'}_{LS,i} \right) \right).
\end{align}
Then,
\begin{align}
\begin{split}
&E\left( \sum^{N}_{i=1}\left( \hat{\boldsymbol{\theta}}_{LS,i}- \hat{\boldsymbol{\theta}}_{LSMG}\right)\left( \hat{\boldsymbol{\theta}}_{LS,i}- \hat{\boldsymbol{\theta}}_{LSMG}\right)^{'} \right)= \\
&\left(1-\frac{1}{N} \right)\sum^{N}_{i=1}Var\left(\hat{\boldsymbol{\theta}}_{LS,i}\right)+ \sum^{N}_{i=1}E \left(\hat{\boldsymbol{\theta}}_{LS,i}\hat{\boldsymbol{\theta}}^{'}_{LS,i} \right)- \frac{1}{N}\sum^{N}_{i=1}\sum^{N}_{j=1}E \left(\hat{\boldsymbol{\theta}}_{LS,i}\hat{\boldsymbol{\theta}}^{'}_{LS,i} \right). \label{24}
\end{split}
\end{align}
From above equation $(\ref{24})$, we can observe the bias term as,
\begin{align}
\aleph=\sum^{N}_{i=1}E \left(\hat{\boldsymbol{\theta}}_{LS,i}\hat{\boldsymbol{\theta}}^{'}_{LS,i} \right)- \frac{1}{N}\sum^{N}_{i=1}\sum^{N}_{j=1}E \left(\hat{\boldsymbol{\theta}}_{LS,i}\hat{\boldsymbol{\theta}}^{'}_{LS,i} \right). \label{25}
\end{align}

 Taking expectation on equation $(26)$, we have
\begin{align}
E\left( \hat{\boldsymbol{\theta}}_{LS,i} \right)=\boldsymbol{\theta}+E \left(b_{i} \right),
\end{align}
From equation $(\ref{25})$, we know that
 \begin{align}
\aleph=\sum^{N}_{i=1}E \left(b_{i}\right) E\left(b^{'}_{i} \right)- \frac{1}{N}\sum^{N}_{i=1}\sum^{N}_{j=1}E \left(b_{i}\right) E\left(b^{'}_{j} \right).
 \end{align}
When $T \to \infty$, $\aleph=0$.
Therefore, we have following theorem \\

\begin{myTheo}
When $(T,N) \overset{j}{\to} \infty$ such that $N/T \to c$ with $0<c< \infty$,
\begin{enumerate}
\item \begin{align}
\sqrt{N}\left(\hat{\boldsymbol{\theta}}_{LSMG}-\boldsymbol{\theta}  \right)\overset{d}{\to} N\left(\boldsymbol{0},\boldsymbol{\Sigma}_{LS,\lambda} \right).
\end{align}
\item \begin{align}
\hat{\boldsymbol{\Sigma}}_{LS,\lambda} \overset{p}{\to}  \boldsymbol{\Sigma}_{LS,\lambda}
\end{align}
where
\begin{align}
 \hat{\boldsymbol{\Sigma}}_{LS,\lambda}=\frac{1}{N-1}\sum^{N}_{i=1}\left( \hat{\boldsymbol{\theta}}_{LS,i}- \hat{\boldsymbol{\theta}}_{LSMG}\right)\left( \hat{\boldsymbol{\theta}}_{LS,i}- \hat{\boldsymbol{\theta}}_{LSMG}\right)^{'}.
\end{align}
\end{enumerate}
\end{myTheo}



\subsection{Asymptotic property of IV estimator}
We use current and lagged values of $\boldsymbol{x}_{i}$ as instruments, as
\begin{align}
\tilde{\boldsymbol{Z}}_{i,t}=\left( \tilde{\boldsymbol{x}}_{i}, \tilde{\boldsymbol{x}}_{i,-1} \right)^{'},
\end{align}
where $\tilde{\boldsymbol{Z}}_{i}$ is $T \times  2$ vector. \\

\begin{myAssu}
$\boldsymbol{A}_{i}=\plim_{T \to \infty}\tilde{\boldsymbol{A}}_{i,T}$ has full column rank, $\boldsymbol{B}_{i}=\plim_{T \to \infty}\tilde{\boldsymbol{B}}_{i,T}$ and $\boldsymbol{\Sigma}_{i}=\plim_{T \to \infty} T^{-1}\boldsymbol{Z}^{'}_{i}\boldsymbol{u}_{i} \boldsymbol{u}_{i}^{'}\boldsymbol{Z}$ has positive definite, uniformly, where $\tilde{\boldsymbol{A}}_{i,T}= \frac{1}{T}\tilde{\boldsymbol{Z}}^{'}_{i}\tilde{\boldsymbol{W}}_{i}$ and $\tilde{\boldsymbol{B}}_{i,T}= \frac{1}{T}\tilde{\boldsymbol{Z}}^{'}_{i}\tilde{\boldsymbol{Z}}_{i}$ \\
\end{myAssu}

Then, the IV estimator can be expressed as
\begin{align}
\boldsymbol{\hat{\theta}}_{IV,i}=\left(\tilde{\boldsymbol{A}}_{i,T}^{'}\tilde{\boldsymbol{B}}_{i,T}^{-1}\tilde{\boldsymbol{A}}_{i,T} \right)\tilde{\boldsymbol{A}}_{i,T}^{'}\tilde{\boldsymbol{B}}_{i,T}^{-1}\tilde{\boldsymbol{g}}_{i,T},
\end{align}
where
\begin{align}
\tilde{\boldsymbol{g}}_{i,T}=\frac{1}{T}\tilde{\boldsymbol{Z}}^{'}_{i}\tilde{\boldsymbol{y}}_{i},
\end{align}
 and $\tilde{\boldsymbol{W}}_{i}=\left( \tilde{w}^{'}_{i,1},\ldots, \tilde{w}^{'}_{i,T} \right)^{'}$ is $T \times 2$ matrix

From above equation, we have
\begin{align}
\sqrt{T}\left(\boldsymbol{\hat{\theta}}_{IV,i}-\boldsymbol{\theta}_{i} \right)=\left(\tilde{\boldsymbol{A}}_{i,T}^{'}\tilde{\boldsymbol{B}}_{i,T}^{-1}\tilde{\boldsymbol{A}}_{i,T} \right)\tilde{\boldsymbol{A}}_{i,T}^{'}\tilde{\boldsymbol{B}}_{i,T}^{-1}\left(T^{-1/2}  \tilde{\boldsymbol{Z}}^{'}_{i}\tilde{\boldsymbol{u}}_{i} \right)
\end{align}
Then, the property of $T^{-1/2}  \tilde{\boldsymbol{Z}}^{'}_{i}\tilde{\boldsymbol{u}}_{i} $ is given by following proposition.\\
\begin{myProp}
Under above assumptions, as $\left(N, T \right)\overset{j}{\to} \infty$ such that $N/T \to c$ with $0 < c<\infty $, for each $i$, we have
\begin{align}
\begin{split}
&N^{-1} \tilde{\boldsymbol{Z}}^{'}_{i}\tilde{\boldsymbol{u}}_{i}\overset{p}{\to} \boldsymbol{0},  \\
&\,\,\,\,and\,\,\,\,\, \\
&T^{-1/2}  \tilde{\boldsymbol{Z}}^{'}_{i}\tilde{\boldsymbol{u}}_{i} \overset{d}{\to} N\left(\boldsymbol{0},\boldsymbol{\Sigma}_{i} \right).
\end{split}
\end{align}
\end{myProp}

Thus, IV  estimator, $\boldsymbol{\theta}_{IV, i}$ is $\sqrt{T}$ consistent to $\boldsymbol{\theta}_{i}$ and this estimator does not have Nickell's bias. Then, we have following theorem\\
$\mathbf{Theorem \,\,1}$ \\
As $\left(N,T \right)\to \infty $ such that $N/T \to c$ with $0< c< \infty$. for each $i$,
\begin{align}
\sqrt{T}\left(\hat{\boldsymbol{\theta}}_{IV,i}-\boldsymbol{\theta}_{i}  \right)\overset{d}{\to} N \left(\boldsymbol{0},\,\left(\boldsymbol{A}_{i}^{'}\boldsymbol{B}_{i}^{-1}\boldsymbol{A}_{i} \right)^{-1}\boldsymbol{A}_{i}^{'}\boldsymbol{B}_{i}^{-1}   \boldsymbol{\Sigma}_{i}\boldsymbol{B}_{i}^{-1} \boldsymbol{A}_{i}\left(\boldsymbol{A}_{i}^{'}\boldsymbol{B}_{i}^{-1}\boldsymbol{A}_{i} \right)\right).
\end{align}


\subsubsection{Mean group IV estimator }
Now, we define the mean group estimator of $\boldsymbol{\theta}$:
\begin{align}
\hat{\boldsymbol{\theta}}_{IVMG}=\frac{1}{N}\sum^{N}_{i=1}\hat{\boldsymbol{\theta}}_{IVi}.
\end{align}
And we can show that the asymptotic property of $\hat{\boldsymbol{\theta}}_{IVMG}$, as
\begin{align}
\hat{\boldsymbol{\theta}}_{IVMG}-\boldsymbol{\theta}=\frac{1}{N}\sum^{N}_{i=1}\left(\hat{\boldsymbol{\theta}}_{IV,i}- \boldsymbol{\theta}\right)=\frac{1}{N}\sum^{N}_{i=1}\left(\hat{\boldsymbol{\theta}}_{IV,i}- \boldsymbol{\theta}_{i}\right)+\frac{1}{N}\sum^{N}_{i=1}\boldsymbol{\lambda}_{i},
\end{align}
where
\begin{align}
\begin{split}
\frac{1}{N}\sum^{N}_{i=1}\left(\hat{\boldsymbol{\theta}}_{IV,i}- \boldsymbol{\theta}_{i}\right)&=\frac{1}{N}\sum^{N}_{i=1} \left(\tilde{\boldsymbol{A}}_{i,T}^{'}\tilde{\boldsymbol{B}}_{i,T}^{-1}\tilde{\boldsymbol{A}}_{i,T} \right)^{-1}\tilde{\boldsymbol{A}}_{i,T}^{'}\tilde{\boldsymbol{B}}_{i,T}^{-1}\left(T^{-1}  \tilde{\boldsymbol{Z}}^{'}_{i}\tilde{\boldsymbol{u}}_{i} \right) \\
&= O_{p}\left(\delta^{-2}_{NT} \right),
\end{split}
\end{align}
where $\delta_{NT}=\min \{\sqrt{N},\,\sqrt{T} \}$.
We note that $\frac{1}{\sqrt{N}}\sum^{N}_{i=1}\boldsymbol{\lambda}_{i}=O_{p}\left(N^{-1/2} \right)$, if $\delta_{NT}^{-(2+\varsigma)}\to 0$ for any $\varsigma >0$, we have
\begin{align}
\sqrt{N}\left(\hat{\boldsymbol{\theta}}_{IVMG}-\boldsymbol{\theta}  \right)=\frac{1}{\sqrt{N}}\sum^{N}_{i=1}\boldsymbol{\lambda}_{i}+o_{p} \left(1 \right).
\end{align}

And the variance estimator of $\hat{\boldsymbol{\theta}}_{IVMG}$ is given by
\begin{align}
\hat{\boldsymbol{\Sigma}}_{IV,\lambda}=\frac{1}{N-1}\sum^{N}_{i=1}\left( \hat{\boldsymbol{\theta}}_{IV,i}- \hat{\boldsymbol{\theta}}_{IVMG}\right)\left( \hat{\boldsymbol{\theta}}_{IV,i}- \hat{\boldsymbol{\theta}}_{IVMG}\right)^{'}. \label{44}
\end{align}
Follow \citet{Norkute:2019}, we can show that $\hat{\boldsymbol{\Sigma}}_{IV,\lambda}$ is consistent and it does not have small $T$ bias.
Firstly, we decompose $(\ref{44})$ as
\begin{align}
\begin{split}
&\sum^{N}_{i=1}\left( \hat{\boldsymbol{\theta}}_{IV,i}- \boldsymbol{\theta}+\boldsymbol{\theta}-    \hat{\boldsymbol{\theta}}_{IVMG}\right)\left( \hat{\boldsymbol{\theta}}_{IV,i}- \boldsymbol{\theta}+\boldsymbol{\theta}-    \hat{\boldsymbol{\theta}}_{IVMG}\right)^{'}= \\
& \sum^{N}_{i=1} \boldsymbol{\lambda}_{i}\boldsymbol{\lambda}^{'}_{i}+\sum^{N}_{i=1}\left(\hat{\boldsymbol{\theta}}_{IV,i}-\boldsymbol{\theta}_{i} \right)\left(\hat{\boldsymbol{\theta}}_{IV,i}-\boldsymbol{\theta}_{i} \right)^{'}+\sum^{N}_{i=1}\left(\hat{\boldsymbol{\theta}}_{IV,i}-\boldsymbol{\theta}_{i}  \right) \boldsymbol{\lambda}_{i}+\sum^{N}_{i=1}\boldsymbol{\lambda}_{i} \left(\hat{\boldsymbol{\theta}}_{IV,i}-\boldsymbol{\theta}_{i}  \right) -\\
&N\left(\boldsymbol{\theta}-\hat{\boldsymbol{\theta}}_{IVMG} \right)^{'}\left(\boldsymbol{\theta}-\hat{\boldsymbol{\theta}}_{IVMG} \right). \label{45}
\end{split}
\end{align}
Then we can show consistent of $\hat{\boldsymbol{\Sigma}}_{IV,\lambda}$ as
\begin{align}
\begin{split}
&\hat{\boldsymbol{\Sigma}}_{IV,\lambda}-\boldsymbol{\Sigma}_{IV,\lambda}=
 \frac{1}{N-1}\sum^{N}_{i=1}\left( \boldsymbol{\lambda}_{i}\boldsymbol{\lambda}^{'}_{i}-\boldsymbol{\Sigma}_{IV,\lambda}\right) +\frac{1}{N-1}\sum^{N}_{i=1}\left(\hat{\boldsymbol{\theta}}_{IV,i}-\boldsymbol{\theta}_{i} \right)\left(\hat{\boldsymbol{\theta}}_{IV,i}-\boldsymbol{\theta}_{i} \right)^{'} \\
&+\frac{1}{N-1}\sum^{N}_{i=1}\left(\hat{\boldsymbol{\theta}}_{IV,i}-\boldsymbol{\theta}_{i}  \right) \boldsymbol{\lambda}_{i}+\frac{1}{N-1}\sum^{N}_{i=1}\boldsymbol{\lambda}_{i} \left(\hat{\boldsymbol{\theta}}_{IV,i}-\boldsymbol{\theta}_{i}  \right) -\\
&\frac{N}{N-1}\left(\boldsymbol{\theta}-\hat{\boldsymbol{\theta}}_{IVMG} \right)^{'}\left(\boldsymbol{\theta}-\hat{\boldsymbol{\theta}}_{IVMG} \right)  =o_{p}(1).
\end{split}
\end{align}






Then, we can see that the asymptotic property of $\hat{\boldsymbol{\theta}}_{IVMG} $ as,
\begin{align}
\sqrt{N}\left(\hat{\boldsymbol{\theta}}_{IVMG}-\boldsymbol{\theta}  \right)\overset{d}{\to} N\left(\boldsymbol{0},\boldsymbol{\Sigma}_{IV,\lambda} \right).
\end{align}




\section{Estimation method on dynamic heterogeneous panel data model with  multifactor error structure}
For convenient, we assume the number of regressor is $1$ and we express the model as
\begin{align}
y_{i,t}=\phi_{i} y_{i,t-1}+ \sum^{k}_{\ell=1}\beta_{\ell i}x_{\ell i,t}+u_{i,t}, \,\, for\,\,i=1,\ldots N;\,t=1,\ldots,T, \, \ell=1,\ldots, k. \label{46}
\end{align}
Consider the model $(\ref{45})$, we drawn $x_{\ell i,t}$ as
\begin{align}
x_{\ell i,t}=\boldsymbol{\gamma}^{0'}_{xi}\boldsymbol{f}^{0}_{xt}+\varepsilon_{xi,t}
\end{align}
and the idiosyncratic errors of the process for $y_{i,t}$ as
\begin{align}
u_{i,t}=\boldsymbol{\gamma}^{0'}_{yi}\boldsymbol{f}^{0}_{yt}+\varepsilon_{yi,t},
\end{align}
where $\boldsymbol{\gamma}^{0}_{yi}$ and $\boldsymbol{\gamma}^{0}_{xi}$ are $m_{y}\times 1$ and $m_{x}\times 1$ true
 factor loading respectively, $\boldsymbol{f}^{0}_{yt}$  and  $\boldsymbol{f}^{0}_{xt}$ are  $m_{y}\times 1$ and $m_{x}\times 1$ true vector of unobservable factors respectively.
\subsection{Norkutes' (2019) IVMG estimator}
 We asymptotically eliminate the common factor in $\boldsymbol{x}_{i}$ by projecting matrix, $\boldsymbol{M}_{F^{0}_{x}}$.
 \begin{align}
 \boldsymbol{M}_{F^{0}_{x}}=\boldsymbol{I}_{T}-\boldsymbol{F}^{0}_{x}\left(\boldsymbol{F}^{0'}_{x}\boldsymbol{F}^{0}_{x}  \right)^{-1}\boldsymbol{F}^{0'}_{x} ; \boldsymbol{M}_{F^{0}_{x,-1}}=\boldsymbol{I}_{T}-\boldsymbol{F}^{0}_{x,-1}\left(\boldsymbol{F}^{0'}_{x,-1}\boldsymbol{F}^{0}_{x,-1}  \right)^{-1}\boldsymbol{F}^{0'}_{x,-1}
 \end{align}
And using the defactored covariates as instruments, as
\begin{align}
\boldsymbol{Z}_{IVi}=\left(\boldsymbol{M}_{F^{0}_{X}}\boldsymbol{x}_{i}, \boldsymbol{M}_{F^{0}_{x,-1}}\boldsymbol{X}_{i,-1}  \right)
\end{align}

The first step IV estimator can be expressed as
\begin{align}
\begin{split}
\boldsymbol{\hat{\varphi}}_{IVi}&=\left(\left(\frac{\boldsymbol{Z}^{'}_{i}\boldsymbol{M}_{F^{0}_{X}}\boldsymbol{W}_{i}}{T } \right)^{'} \left(\frac{\boldsymbol{Z}^{'}_{i}\boldsymbol{M}_{F^{0}_{X}}\boldsymbol{Z}_{i}}{T}  \right)^{-1}  \left(\frac{\boldsymbol{Z}^{'}_{i}\boldsymbol{M}_{F^{0}_{X}}\boldsymbol{W}_{i}}{T}  \right)\right)^{-1} \\
&\left(\left( \frac{\boldsymbol{Z}^{'}_{i}\boldsymbol{M}_{F^{0}_{X}}\boldsymbol{W}_{i}}{T}  \right)^{'} \left(\frac{\boldsymbol{Z}^{'}_{i}\boldsymbol{M}_{F^{0}_{X}}\boldsymbol{Z}_{i}}{T}  \right)^{-1}  \left(\frac{\boldsymbol{Z}^{'}_{i}\boldsymbol{M}_{F^{0}_{X}}\boldsymbol{y}_{i}}{T}  \right)\right).
\end{split}
\end{align}







\section{Monte Carlo simulation design}
\subsection{dynamic heterogeneous panels data model without error factor structure}
The data generating process:
\begin{align}
\begin{split}
y_{i,t}&= \phi_{i} y_{i,t-1}+ \sum^{k}_{\ell=1}\beta_{\ell i}x_{\ell i,t}+u_{i,t}, \,\, for\,\,i=1,\ldots N;\,t=-49,\ldots,T\, , \\ \label{M1}
x_{\ell i,t}&=\sum^{k}_{\ell=1}\phi_{\ell i}x_{\ell i,t-1}+v_{\ell i,t},
\end{split}
\end{align}
where $u_{i,t}\overset{i.i.d.}{\sim} \mathcal{N}(0,\,1)\,,$ and $v_{\ell i,t}=\rho_{v, \ell}v_{\ell i,t-1}+\left( 1-\rho^{2}_{v, \ell} \right)^{\frac{1}{2}}\varpi_{\ell i,t}, \varpi_{\ell i,t} \overset{i.i.d.}{\sim} U(0.5, 1.5) \, ,\rho_{v, \ell}=0.5.$

The slope coefficients are generated as
\begin{align}
\phi_{i}=\phi+\eta_{\phi i},\,\, \beta_{1,i}=\beta_{1}+\eta_{\beta_{1} i}\, and\, \beta_{2,i}=\beta_{2}+\eta_{\beta_{2}i}.
\end{align}
Here we consider $\phi \in \left\{0.5 \right\}$, $\beta_{1}=3$ and $\beta_{2}=1$. For the design of heterogenous slopes, $\eta_{\phi i} \overset{i.i.d.}{\sim} U\left( -c, c\right)$, and
\begin{align}
\eta_{\beta_{\ell}i}= \left(1-\rho^{2}_{\beta}  \right)^{1/2}\eta_{\phi i}.
\end{align}
Here, we set $c=0.2,\, \rho_{\beta}=0.4$ for $\ell=1,2.$


\subsection{Dynamic heterogeneous panels data model with multi-factor error structure}
This Monte Carlo simulation design same as \citet{Norkute:2019}.
For convenience, we rewrite the data generating process as bellow
\begin{align}
y_{i,t}=\alpha_{i}+\phi_{i} y_{i,t-1}+ \sum^{k}_{\ell=1}\beta_{\ell i}x_{\ell i,t}+u_{i,t}, \,\, for\,\,i=1,\ldots N;\,t=-49,\ldots,T\, . \\ \label{M1}
\end{align}
We allow error factor structure in the model as
\begin{align}
u_{i,t}=    \sum^{m_{y}}_{s=1}\gamma^{0}_{si}f^{0}_{s,t} +\varepsilon_{i,t},
\end{align}
where
\begin{align}
f^{0}_{s,t}=\rho^{0}_{s,t} f^{0}_{s,t-1}+\left( 1-\rho^{2}_{fs} \right)^{1/2}\zeta_{s,t},
\end{align}
with $\zeta_{s,t} \overset{i.i.d.}{\sim} N(0,1)$ for $s=1,\ldots m_{y}$. We assume $k=2$ and $m_{y}=1+k=3$ and set $\rho^{0}_{s,t}=0.5$ for all $s$. The error term, $\varepsilon_{i,t}$, setting as
\begin{align}
\varepsilon_{i,t}=\varsigma_{\varepsilon}\sigma_{it}\left(\epsilon_{it}-1 \right)/\sqrt{2},
\end{align}
where $ \epsilon_{it}\overset{i.i.d.}{\sim} \chi^{2}_{1},\, \sigma^{2}_{it}=\eta_{i}\varphi_{t}, \,\eta_{i}\overset{i.i.d.}{\sim}\chi^{2}_{2}/2,$  and  $\varphi_{t}=t/T\, for \,\, t=0,\ldots, T.$ And we set
\begin{align}
\varsigma_{\varepsilon}=\frac{\pi_{\mu}}{1-\pi_{\mu}}m_{y}.
\end{align}
we set $\pi_{\mu} \in \left\{ 3/4\right\}.$

The process of regressors is
\begin{align}
x_{\ell it}=\mu_{\ell i}++\sum^{k}_{\ell=1}\phi_{\ell i}x_{\ell i,t-1}+\sum^{m_{x}}_{s=1}\gamma^{0}_{\ell si}f^{0}_{s,t}+v_{\ell it}, \,\, for\,\,i=1,\ldots N;\,t=-49,\ldots,T\,; \, \ell=1,2.
\end{align}
We set number of factor, $m_{x}$, is $2$. Therefore, $\boldsymbol{f}^{0}_{y,t}=\left(f^{0}_{1t}, f^{0}_{2t},f^{0}_{3t}  \right)^{'}$ and $\boldsymbol{f}^{0}_{x,t}=\left(f^{0}_{1t}, f^{0}_{2t}  \right)^{'}.$
We set
\begin{align}
v_{\ell i,t}=\rho_{v, \ell}v_{\ell i,t-1}+\left( 1-\rho^{2}_{v, \ell} \right)^{\frac{1}{2}}\varpi_{\ell i,t},  for \, \ell=1,2,
\end{align}
  where $\rho_{v, \ell}=0.5$ for all $\ell$.
The individual effect is
\begin{align}
 \alpha^{\ast}_{i}\overset{i.i.d.}{\sim} N\left(0, (1-\rho_{i})^{2} \right), \,\, \mu^{\ast}_{\ell i}=\rho_{\mu,\ell}\alpha^{\ast}_{i}+\left( 1-\rho^{2}_{\mu,\ell} \right)^{1/2}\omega_{\ell i},
\end{align}
where $\omega \overset{i.i.d.}{\sim}N\left(0, (1-\rho_{i})^{2} \right)$ and $\rho_{\mu,\ell}=0.5$.

Now, we define the factor loading in $u_{i,t}$ are generated as $\gamma^{0\ast}_{si}\overset{i.i.d.}{\sim}N\left(0, 1 \right)$, for $s=1,\ldots, m_{y}=3$, and the factor loading in $x_{1it}$ and $x_{2it}$ are drawn as
\begin{align}
\begin{split}
\gamma^{0\ast}_{1si}&=\rho_{\gamma,1s} \gamma^{0\ast}_{3i}+\left(1-\rho^{2}_{\gamma,1s}  \right)^{1/2}\xi_{1si};\,\xi_{1si}\overset{i.i.d.}{\sim}N\left(0, 1 \right); \\
\gamma^{0\ast}_{2si}&=\rho_{\gamma,2s} \gamma^{0\ast}_{si}+\left(1-\rho^{2}_{\gamma,2s}  \right)^{1/2}\xi_{2si};\,\xi_{2si}\overset{i.i.d.}{\sim}N\left(0, 1 \right);
\end{split}
\end{align}
for $s=1,\ldots, m_{x}=2$. We set $\rho_{\gamma, 11}=\rho_{\gamma, 12} \in \left\{ 0.5 \right\}$ and $\rho_{\gamma, 21}=\rho_{\gamma, 22}=0.5.$
The factor loading are generated as
\begin{align}
\boldsymbol{\Gamma}=\boldsymbol{\Gamma}^{0}+\boldsymbol{\Gamma}^{0\ast}_{i}
\end{align}
where
\begin{align}
\boldsymbol{\Gamma}^{0}_{i}=
\begin{bmatrix}
\gamma^{0}_{1i} & \gamma^{0}_{11i} & \gamma^{0}_{21i} \\
\gamma^{0}_{2i} & \gamma^{0}_{12i} & \gamma^{0}_{22i} \\
\gamma^{0}_{3i} &  0               &     0
\end{bmatrix}
\end{align}
and
\begin{align}
\boldsymbol{\Gamma}^{0\ast}_{i}=
\begin{bmatrix}
\gamma^{0\ast}_{1i} & \gamma^{0\ast}_{11i} & \gamma^{0\ast}_{21i} \\
\gamma^{0\ast}_{2i} & \gamma^{0\ast}_{12i} & \gamma^{0\ast}_{22i} \\
\gamma^{0\ast}_{3i} &  0               &     0
\end{bmatrix}.
\end{align}
We set
\begin{align}
\boldsymbol{\Gamma}^{0}=
\begin{bmatrix}
1/4 & 1/4 & -1 \\
1/2 & -1  & 1/4 \\
1/2 & 0   & 0
\end{bmatrix} .
\end{align}
And
\begin{align}
\alpha_{i}=\alpha+ \alpha^{\ast}_{i}, \, \mu_{\ell i}= \mu_{\ell}+\mu^{\ast}_{\ell i},
\end{align}
where $\alpha=1/2$, $\mu_{1}=1$, $\mu_{2}=-1/2$.

The slope coefficients are generated as
\begin{align}
\phi_{i}=\phi+\eta_{\phi i},\,\, \beta_{1,i}=\beta_{1}+\eta_{\beta_{1} i}\, and\, \beta_{2,i}=\beta_{2}+\eta_{\beta_{2}i}.
\end{align}
Here we consider $\phi \in \left\{0.5\right\}$, $\beta_{1}=3$ and $\beta_{2}=1$. For the design of heterogenous slopes, $\eta_{\phi i} \overset{i.i.d.}{\sim} U\left( -c, c\right)$, and
\begin{align}
\eta_{\beta_{\ell}i}=\left[(2c)^{2}/12 \right]\rho_{\beta}\xi_{\beta \ell i}+ \left(1-\rho^{2}_{\beta}  \right)^{1/2}\eta_{\phi i},
\end{align}
where
\begin{align}
\xi_{\beta \ell i}=\frac{\bar{v^{2}_{\ell i}}- \bar{v^{2}_{\ell}}}{\left[ N^{-1}\sum^{N}_{i=1} \left( \bar{v^{2}_{\ell i}}- \bar{v^{2}_{\ell }}\right)^{2} \right]^{1/2} },
\end{align}
with $\bar{v^{2}_{ell i}}=T^{-1}\sum^{T}_{t=1}v^{2}_{\ell i t}$, $\bar{v^{2}_{\ell}}=N^{-1} \sum^{N}_{i=1} \bar{v^{2}_{\ell i}}$, for $\ell=1,2.$
Here, we set $c=0.2,\, \rho_{\beta}=0.4$ for $\ell=1,2.$ And
\begin{align}
\varsigma^{2}_{v}=\varsigma^{2}_{\varepsilon}\left[SNR-\frac{\rho^{2}_{v}}{1-\rho^{2}_{v}}   \right]\left(\frac{\beta^{2}_{1}+\beta^{2}_{2}}{1-\rho^{2}_{v}}  \right)^{-1},
\end{align}

where $SNR=4$. For the $(T,N)$, we consider $T \in \left\{25, 50, 100, 200  \right\}$ and  $N \in \left\{25, 50, 100, 200  \right\}.$




\section{Monte Carlo simulation results}
\subsection{Dynamic Heterogeneous Panels without multifactor error structure }
We consider ARDL(1,0) model. \\
$\phi \in \left\{0.5 \right\}.$ \\
 $\beta_{1}=3.$ \\
  $\beta_{2}=1.$ \\
$u_{i,t}\overset{i.i.d.}{\sim} \mathcal{N}(0,\,1).$ \\
$\varpi_{\ell i,t} \overset{i.i.d.}{\sim} U(0.5, 1.5).$ \\
$\rho_{v, \ell}=0.5.$ \\
  $c=0.2.$ \\
  $ \rho_{\beta}=0.4.$ \\
 $T \in \left\{25, 50, 100, 200  \right\}.$ \\
 $N \in \left\{25, 50, 100, 200  \right\}.$ \\

LSMG estimator is provided in sheet 1 of MC.xlsx file. \\
IVMG estimator is provided in sheet 2 of MC.xlsx file. \\




\subsection{Dynamic Heterogeneous Panels with multifactor error structure }
We consider ARDL(1,0) model. \\
$\phi \in \left\{0.5 \right\}.$ \\
 $\beta_{1}=3.$ \\
  $\beta_{2}=1.$ \\
  $k=2.$ \\
  $m_{y}=1+k=3.$\\
  $m_{x}=k=2.$\\
$\zeta_{s,t} \overset{i.i.d.}{\sim} N(0,1)$ \\
$\pi_{\mu} \in \left\{ 3/4\right\}.$ \\
$\rho^{0}_{s,t}=0.5.$ \\
$\rho_{v, \ell}=0.5.$\\
 $\rho_{\mu,\ell}=0.5.$ \\
  $\gamma^{0\ast}_{si}\overset{i.i.d.}{\sim}N\left(0, 1 \right).$ \\
 $\xi_{1si}\overset{i.i.d.}{\sim}N\left(0, 1 \right).$ \\
$\xi_{2si}\overset{i.i.d.}{\sim}N\left(0, 1 \right).$ \\
$\rho_{\gamma, 11}=\rho_{\gamma, 12} \in \left\{ 0.5 \right\}.$ \\
$\rho_{\gamma, 21}=\rho_{\gamma, 22}=0.5.$ \\
$
\boldsymbol{\Gamma}^{0}=
\begin{bmatrix}
1/4 & 1/4 & -1 \\
1/2 & -1  & 1/4 \\
1/2 & 0   & 0
\end{bmatrix} .
$ \\
$\alpha=1/2.$ \\
 $\mu_{1}=1.$\\
  $\mu_{2}=-1/2.$ \\
$c=0.2.$ \\
$\rho_{\beta}=0.4.$ \\
$SNR=4.$ \\
$T \in \left\{25, 50, 100, 200  \right\}.$ \\
 $N \in \left\{25, 50, 100, 200  \right\}.$ \\

IVMG estimator is provided in sheet 3 of MC.xlsx file. \\

\section{Short summary}
\subsection{Dynamic Heterogeneous Panels without multifactor error structure}
1. The performance of IVMG estimator is better than LSMG estimator in bias and RMSE.


\subsection{Dynamic Heterogeneous Panels with multifactor error structure}
1. When $N$ and $T$ increase, the performance of IVMG estimator is good in bias and RMSE. \\
Related literature to dynamic Heterogeneous Panels with multifactor error structure: \citet{Chudik:2015} and \citet{Norkute:2019}.\\
Related literature to choosing number of instruments: \citet{Stephen:2001}, \citet{Swanson:2005}, \citet{Marine:2012}, \citet{Bai:2010} and \citet{Kang:2019}.


\newpage

\addcontentsline{toc}{section}{Reference}
\renewcommand\refname{References}
\bibliographystyle{chicago}
\bibliography{1}

\begin{appendices}
\section{Some Notation}

\section{Some More Notation}
\end{appendices}



=======
\documentclass[12pt,a4paper,hyperref]{article}
\usepackage[usenames,dvipsnames]{xcolor}
\definecolor{darkblue}{rgb}{0.0, 0.0, 0.55}
	\definecolor{ultramarine}{rgb}{0.07, 0.04, 0.56}
\usepackage{amsmath, natbib, latexsym, array, amssymb,longtable,float, graphicx, appendix,lscape,diagbox,textcomp,placeins}
\usepackage[colorlinks,
            linkcolor=ultramarine,
            anchorcolor=green,
            citecolor=darkblue
            ]{hyperref}
\usepackage[flushleft]{threeparttable}
\usepackage[top=2.7cm, left=3cm, right=3cm, bottom=2.7cm]{geometry}
\usepackage{hyperref}
\newtheorem{myDef}{Definition}
\newtheorem{myTheo}{Theorem}
\newtheorem{myProp}{proposition}
\newtheorem{myRem}{Remark}
\newtheorem{myAssu}{Assumption}
\newtheorem{myCor}{Corollary}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}

\urlstyle{same}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{pgfplotstable}
\sisetup{
  round-mode          = places, % Rounds numbers
  round-precision     = 2, % to 2 places
}

\newenvironment{sequation}{\begin{equation}\tiny}{\end{equation}}
\DeclareMathOperator*{\plim}{plim}
\renewcommand{\floatpagefraction}{0.60}
\renewcommand{\appendixpagename}{\Large Appendix}
\setcounter{secnumdepth}{3}
\begin{document}
\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page



\HRule \\[0.4cm]
{ \huge \bfseries Dynamic Heterogeneous Panels }\\[0.4cm] % Title of your document
\HRule \\[1.5cm]


\vfill % Fill the rest of the page with whitespace

\end{titlepage}

\newpage
\tableofcontents
\newpage
\section{Model, asymptotic property of IV estimator}
\subsection{The models and Assumptions}
\subsubsection{Fixed effects model}
Consider the dynamic heterogeneous panels data model with fixed effects:
\begin{align}
\begin{split}
y_{i,t}&=\alpha_{i}+\phi_{i} y_{i,t-1}+ \boldsymbol{x}^{'}_{i,t}\boldsymbol{\beta}_{i}+u_{i,t}, \,\, for\,\,i=1,\ldots N;\,t=1,\ldots,T\, , \\
&= \boldsymbol{w}^{'}_{i,t}\boldsymbol{\theta}_{i}+ \alpha_{i}+u_{i,t}, \label{1}
\end{split}
\end{align}
where $\boldsymbol{x}_{i,t}$ and $\boldsymbol{\beta}_{i}$ are $k \times 1$ vectors, $\boldsymbol{\theta}_{i}=\left(\phi_{i}, \boldsymbol{\beta}^{'}_{i} \right)^{'}$ and $\boldsymbol{w}_{i,t}=\left(y_{i,t-1}, \boldsymbol{x}^{'}_{i,t} \right)^{'}$ is a $\left(1+k\right) \times 1$ vector.
Stacking the $T$ observations for each $i$, we have
\begin{align}
\boldsymbol{y}_{i}=\boldsymbol{W}_{i}\boldsymbol{\theta}_{i}+ \alpha_{i}\boldsymbol{\iota}_{T}+ \boldsymbol{u}_{i}, \label{2}
\end{align}
where $\boldsymbol{y}_{i}=\left(y_{i,1},\ldots, y_{i,T} \right)^{'}$, $\boldsymbol{W}_{i}=\left(\boldsymbol{w}_{i,1},\ldots, \boldsymbol{w}_{i,T} \right)^{'}$, $\iota_{T}=\left(1,\ldots,1 \right)^{'}$ and $\boldsymbol{u}_{i}=\left(u_{i,1}, \ldots, u_{i,T} \right)^{'}$. 

Due to the incidental parameters problem arise, we use forward filter to the model by \citet{Moon:2000}, \citet{Hayakawa:2009} and \citet{Hayakawa:2019}. We define the $\left(T-1 \right) \times 1$ forward demeaning matrix as

\begin{align}
\boldsymbol{F}=diag(c_{1}, c_{2}, \ldots c_{T-1})
\begin{bmatrix}
1 & \frac{-1}{T-1} & \cdots & \cdots & \frac{-1}{T-1}\\
\vdots & 1 & \frac{-1}{T-2} & \cdots & \frac{-1}{T-2}\\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & -1 
\end{bmatrix},
\end{align}    
where $c_{t}=\sqrt{\left( T-t\right)\left( T-t+1\right) }$.
Multiply $\boldsymbol{F}$ to model $(\ref{2})$, the model can expressed as  
\begin{align}
\tilde{\boldsymbol{y}}_{i}= \tilde{\boldsymbol{W}}_{i}\boldsymbol{\theta}+\tilde{\boldsymbol{u}}_{i},
\end{align}
where $\tilde{\boldsymbol{y}}_{i}=\boldsymbol{F}\boldsymbol{y}_{i}= \left(\tilde{\boldsymbol{y}}_{i,1}, \ldots, \tilde{\boldsymbol{y}}_{i,T-1} \right)^{'}$,  $\tilde{\boldsymbol{W}}_{i}=\boldsymbol{F}\boldsymbol{W}_{i}= \left(\tilde{\boldsymbol{w}}_{i,1}, \ldots, \tilde{\boldsymbol{w}}_{i,T-1} \right)^{'}$ and $\tilde{\boldsymbol{u}}_{i}=\boldsymbol{F}\boldsymbol{u}_{i}= \left(\tilde{\boldsymbol{u}}_{i,1}, \ldots, \tilde{\boldsymbol{u}}_{i,T-1} \right)^{'}$ with $\tilde{\boldsymbol{y}}_{i,t}=c_{t}\left[y_{i,t}-\left(y_{i,t+1}+\cdots+y_{i,T} \right)/\left(T-t \right) \right]$, for $t=1, \ldots, T-1.$


\subsubsection{Trend model}
Consider the dynamic heterogeneous panels data model with fixed effects and heterogeneous time trends:
\begin{align}
\begin{split}
y_{i,t}&=\alpha_{i}+\eta_{i}t +\phi_{i} y_{i,t-1}+ \boldsymbol{x}^{'}_{i,t}\boldsymbol{\beta}_{i}+u_{i,t}, \,\, for\,\,i=1,\ldots N;\,t=1,\ldots,T\, , \\
&= \boldsymbol{w}^{'}_{i,t}\boldsymbol{\theta}+ \alpha_{i}+\eta_{i}t+u_{i,t}, 
\end{split}
\end{align}
where $\boldsymbol{x}_{i,t}$ and $\boldsymbol{\beta}_{i}$ are $k \times 1$ vectors and $\boldsymbol{w}_{i,t}=\left(\phi_{i}, \boldsymbol{\beta}^{'}_{i} \right)^{'}$ is a $\left(1+k\right) \times 1$ vector.
Stacking the $T$ observations for each $i$, we have
\begin{align}
\boldsymbol{y}_{i}=\boldsymbol{W}_{i}\boldsymbol{\theta}_{i}+ \alpha_{i}\boldsymbol{\iota}_{T}+\eta_{i}\boldsymbol{\tau}_{T} + \boldsymbol{u}_{i}, \label{6}
\end{align}
where $\boldsymbol{\tau}_{T}=\left(1, 2, \ldots, T \right)^{'}$. Again, we define the forward demeaning matrix as

\begin{align}
\boldsymbol{F}^{\tau}=diag(c^{\tau}_{1}, c^{\tau}_{2}, \ldots c^{\tau}_{T-2})
\begin{bmatrix}
1 & \frac{2(-2(T-2))}{(T-1)(T-2)} & \frac{2(-2(T-2)+3)}{(T-1)(T-2)} & \cdots & \frac{2(-2(T-2)+3(T-2))}{(T-1)(T-2)} \\
0 & 1 & \frac{2(-2(T-3))}{(T-2)(T-3)} & \cdots & \frac{2(-2(T-3)+3(T-3))}{(T-3)(T-4)} \\
\vdots &\vdots & \vdots & \ddots  &\vdots \\
0 & \cdots & \cdots & \cdots & \frac{2(-2+3)}{2 \cdot 1}
\end{bmatrix}, \label {7}
\end{align}   
where $c^{\tau}_{t}=\sqrt((T-t)(T-t-1)/(T-t-1)(T-t+2)).$

Multiply $\boldsymbol{F}$ to model $(\ref{6})$, the model can expressed as  
\begin{align}
\tilde{\boldsymbol{y}}^{\tau}_{i}= \tilde{\boldsymbol{W}}^{\tau}_{i}\boldsymbol{\theta}+\tilde{\boldsymbol{u}}^{\tau}_{i},
\end{align}
where $\tilde{\boldsymbol{y}}^{\tau}_{i}=\boldsymbol{F}^{\tau}\boldsymbol{y}_{i}= \left(\tilde{\boldsymbol{y}}^{\tau}_{i,1}, \ldots, \tilde{\boldsymbol{y}}^{\tau}_{i,T-2} \right)^{'}$,  $\tilde{\boldsymbol{W}}^{\tau}_{i}=\boldsymbol{F}^{\tau}\boldsymbol{W}_{i}$ and $\tilde{\boldsymbol{u}}^{\tau}_{i}=\boldsymbol{F}^{\tau}\boldsymbol{u}_{i}.$







\begin{myAssu}
$x_{i,t}$ and $u_{i,t}$ are independently distributed for all $t$ and $s$ .
\end{myAssu}







\subsection{IV estimation method and asymptotic property}
\citet{Norkute:2019} propose an IV estimator in dynamic heterogeneous panel data model. 
They use current and lagged values of $\boldsymbol{x}_{i}$ as instruments, as
\begin{align}
\boldsymbol{Z}_{i,t}=\left( \boldsymbol{x}_{i}, \boldsymbol{x}_{i,-1} \right)^{'},
\end{align}
where $\boldsymbol{Z}_{i}$ is $T \times  2k$ vector. \\

\begin{myAssu}
$\boldsymbol{A}_{i}=\plim_{T \to \infty}\tilde{\boldsymbol{A}}_{i,T}$ has full column rank, $\boldsymbol{B}_{i}=\plim_{T \to \infty}\tilde{\boldsymbol{B}}_{i,T}$ and $\boldsymbol{\Sigma}_{i}=\plim_{T \to \infty} T^{-1}\boldsymbol{Z}^{'}_{i}\boldsymbol{u}_{i} \boldsymbol{u}_{i}^{'}\boldsymbol{Z}$ has positive definite, uniformly, where $\tilde{\boldsymbol{A}}_{i,T}= \frac{1}{T}\boldsymbol{Z}^{'}_{i}\tilde{\boldsymbol{W}}^{b}_{i}$ and $\tilde{\boldsymbol{B}}_{i,T}= \frac{1}{T}\tilde{\boldsymbol{Z}}^{'}_{i}\boldsymbol{Z}_{i}$, \\
where $b = 1$ corresponds to the fixed effects model while $b = 2$ corresponds to the trend models.
\end{myAssu}

Then, the IV estimator can be expressed as
\begin{align}
\boldsymbol{\hat{\theta}}^{b}_{IV,i}=\left(\tilde{\boldsymbol{A}}_{i,T}^{'}\tilde{\boldsymbol{B}}_{i,T}^{-1}\tilde{\boldsymbol{A}}_{i,T} \right)\tilde{\boldsymbol{A}}_{i,T}^{'}\tilde{\boldsymbol{B}}_{i,T}^{-1}\tilde{\boldsymbol{g}}_{i,T},
\end{align}
where
\begin{align}
\tilde{\boldsymbol{g}}_{i,T}=\frac{1}{T}\boldsymbol{Z}^{'}_{i}\tilde{\boldsymbol{y}}^{b}_{i},
\end{align}
 and $\tilde{\boldsymbol{W}}^{b}_{i}=\left( \tilde{w}^{b}_{i,1},\ldots, \tilde{w}^{b}_{i,T} \right)^{'}$ is $T \times 2k$ matrix

From above equation, we have
\begin{align}
\sqrt{T}\left(\boldsymbol{\hat{\theta}}^{b}_{IV,i}-\boldsymbol{\theta}_{i} \right)=\left(\tilde{\boldsymbol{A}}_{i,T}^{'}\tilde{\boldsymbol{B}}_{i,T}^{-1}\tilde{\boldsymbol{A}}_{i,T} \right)\tilde{\boldsymbol{A}}_{i,T}^{'}\tilde{\boldsymbol{B}}_{i,T}^{-1}\left(T^{-1/2}  \boldsymbol{Z}^{'}_{i}\tilde{\boldsymbol{u}}_{i} \right)
\end{align}
Then, the property of $T^{-1/2}  \boldsymbol{Z}^{'}_{i}\tilde{\boldsymbol{u}}_{i} $ is given by following proposition.\\
\begin{myProp}
Under above assumptions, as $\left(N, T \right)\overset{j}{\to} \infty$ such that $N/T \to c$ with $0 < c<\infty $, for each $i$, we have
\begin{align}
\begin{split}
&N^{-1} \boldsymbol{Z}^{'}_{i}\tilde{\boldsymbol{u}}_{i}\overset{p}{\to} \boldsymbol{0},  \\
&\,\,\,\,and\,\,\,\,\, \\
&T^{-1/2}  \boldsymbol{Z}^{'}_{i}\tilde{\boldsymbol{u}}_{i} \overset{d}{\to} N\left(\boldsymbol{0},\boldsymbol{\Sigma}_{i} \right).
\end{split}
\end{align}
\end{myProp}

Thus, IV  estimator, $\boldsymbol{\theta}^{b}_{IV, i}$ is $\sqrt{T}$ consistent to $\boldsymbol{\theta}_{i}$ and this estimator does not have Nickell's bias. Then, we have following theorem\\
$\mathbf{Theorem \,\,1}$ \\
As $\left(N,T \right)\to \infty $ such that $N/T \to c$ with $0< c< \infty$. for each $i$,
\begin{align}
\sqrt{T}\left(\hat{\boldsymbol{\theta}}^{b}_{IV,i}-\boldsymbol{\theta}_{i}  \right)\overset{d}{\to} N \left(\boldsymbol{0},\,\left(\boldsymbol{A}_{i}^{'}\boldsymbol{B}_{i}^{-1}\boldsymbol{A}_{i} \right)^{-1}\boldsymbol{A}_{i}^{'}\boldsymbol{B}_{i}^{-1}   \boldsymbol{\Sigma}_{i}\boldsymbol{B}_{i}^{-1} \boldsymbol{A}_{i}\left(\boldsymbol{A}_{i}^{'}\boldsymbol{B}_{i}^{-1}\boldsymbol{A}_{i} \right)\right).
\end{align}


\subsubsection{Mean group IV estimator }
Now, we define the mean group estimator of $\boldsymbol{\theta}$:
\begin{align}
\hat{\boldsymbol{\theta}}^{b}_{IVMG}=\frac{1}{N}\sum^{N}_{i=1}\hat{\boldsymbol{\theta}}^{b}_{IVi}.
\end{align}
And we can show that the asymptotic property of $\hat{\boldsymbol{\theta}}_{IVMG}$, as
\begin{align}
\hat{\boldsymbol{\theta}}^{b}_{IVMG}-\boldsymbol{\theta}=\frac{1}{N}\sum^{N}_{i=1}\left(\hat{\boldsymbol{\theta}}^{b}_{IV,i}- \boldsymbol{\theta}\right)=\frac{1}{N}\sum^{N}_{i=1}\left(\hat{\boldsymbol{\theta}}^{b}_{IV,i}- \boldsymbol{\theta}_{i}\right)+\frac{1}{N}\sum^{N}_{i=1}\boldsymbol{\lambda}_{i},
\end{align}
where
\begin{align}
\begin{split}
\frac{1}{N}\sum^{N}_{i=1}\left(\hat{\boldsymbol{\theta}}^{b}_{IV,i}- \boldsymbol{\theta}_{i}\right)&=\frac{1}{N}\sum^{N}_{i=1} \left(\tilde{\boldsymbol{A}}_{i,T}^{'}\tilde{\boldsymbol{B}}_{i,T}^{-1}\tilde{\boldsymbol{A}}_{i,T} \right)^{-1}\tilde{\boldsymbol{A}}_{i,T}^{'}\tilde{\boldsymbol{B}}_{i,T}^{-1}\left(T^{-1}  \tilde{\boldsymbol{Z}}^{'}_{i}\tilde{\boldsymbol{u}}_{i} \right) \\
&= O_{p}\left(\delta^{-2}_{NT} \right),
\end{split}
\end{align}
where $\delta_{NT}=\min \{\sqrt{N},\,\sqrt{T} \}$.
We note that $\frac{1}{\sqrt{N}}\sum^{N}_{i=1}\boldsymbol{\lambda}_{i}=O_{p}\left(N^{-1/2} \right)$, if $\delta_{NT}^{-(2+\varsigma)}\to 0$ for any $\varsigma >0$, we have
\begin{align}
\sqrt{N}\left(\hat{\boldsymbol{\theta}}^{b}_{IVMG}-\boldsymbol{\theta}  \right)=\frac{1}{\sqrt{N}}\sum^{N}_{i=1}\boldsymbol{\lambda}_{i}+o_{p} \left(1 \right).
\end{align}

And the variance estimator of $\hat{\boldsymbol{\theta}}_{IVMG}$ is given by
\begin{align}
\hat{\boldsymbol{\Sigma}}_{IV,\lambda}=\frac{1}{N-1}\sum^{N}_{i=1}\left( \hat{\boldsymbol{\theta}}^{b}_{IV,i}- \hat{\boldsymbol{\theta}}^{b}_{IVMG}\right)\left( \hat{\boldsymbol{\theta}}^{b}_{IV,i}- \hat{\boldsymbol{\theta}}^{b}_{IVMG}\right)^{'}. \label{44}
\end{align}
Follow \citet{Norkute:2019}, we can show that $\hat{\boldsymbol{\Sigma}}_{IV,\lambda}$ is consistent and it does not have small $T$ bias.
Firstly, we decompose $(\ref{44})$ as
\begin{align}
\begin{split}
&\sum^{N}_{i=1}\left( \hat{\boldsymbol{\theta}}^{b}_{IV,i}- \boldsymbol{\theta}+\boldsymbol{\theta}-    \hat{\boldsymbol{\theta}}^{b}_{IVMG}\right)\left( \hat{\boldsymbol{\theta}}^{b}_{IV,i}- \boldsymbol{\theta}+\boldsymbol{\theta}-    \hat{\boldsymbol{\theta}}^{b}_{IVMG}\right)^{'}= \\
& \sum^{N}_{i=1} \boldsymbol{\lambda}_{i}\boldsymbol{\lambda}^{'}_{i}+\sum^{N}_{i=1}\left(\hat{\boldsymbol{\theta}}^{b}_{IV,i}-\boldsymbol{\theta}_{i} \right)\left(\hat{\boldsymbol{\theta}}^{b}_{IV,i}-\boldsymbol{\theta}_{i} \right)^{'}+\sum^{N}_{i=1}\left(\hat{\boldsymbol{\theta}}^{b}_{IV,i}-\boldsymbol{\theta}_{i}  \right) \boldsymbol{\lambda}_{i}+\sum^{N}_{i=1}\boldsymbol{\lambda}_{i} \left(\hat{\boldsymbol{\theta}}^{b}_{IV,i}-\boldsymbol{\theta}_{i}  \right) -\\
&N\left(\boldsymbol{\theta}-\hat{\boldsymbol{\theta}}^{b}_{IVMG} \right)^{'}\left(\boldsymbol{\theta}-\hat{\boldsymbol{\theta}}^{b}_{IVMG} \right). \label{45}
\end{split}
\end{align}
Then we can show consistent of $\hat{\boldsymbol{\Sigma}}^{b}_{IV,\lambda}$ as
\begin{align}
\begin{split}
&\hat{\boldsymbol{\Sigma}}^{b}_{IV,\lambda}-\boldsymbol{\Sigma}_{IV,\lambda}=
 \frac{1}{N-1}\sum^{N}_{i=1}\left( \boldsymbol{\lambda}_{i}\boldsymbol{\lambda}^{'}_{i}-\boldsymbol{\Sigma}_{IV,\lambda}\right) +\frac{1}{N-1}\sum^{N}_{i=1}\left(\hat{\boldsymbol{\theta}}^{b}_{IV,i}-\boldsymbol{\theta}_{i} \right)\left(\hat{\boldsymbol{\theta}}^{b}_{IV,i}-\boldsymbol{\theta}_{i} \right)^{'} \\
&+\frac{1}{N-1}\sum^{N}_{i=1}\left(\hat{\boldsymbol{\theta}}^{b}_{IV,i}-\boldsymbol{\theta}_{i}  \right) \boldsymbol{\lambda}_{i}+\frac{1}{N-1}\sum^{N}_{i=1}\boldsymbol{\lambda}_{i} \left(\hat{\boldsymbol{\theta}}^{b}_{IV,i}-\boldsymbol{\theta}_{i}  \right) -\\
&\frac{N}{N-1}\left(\boldsymbol{\theta}-\hat{\boldsymbol{\theta}}^{b}_{IVMG} \right)^{'}\left(\boldsymbol{\theta}-\hat{\boldsymbol{\theta}}^{b}_{IVMG} \right)  =o_{p}(1).
\end{split}
\end{align}






Then, we can see that the asymptotic property of $\hat{\boldsymbol{\theta}}_{IVMG} $ as,
\begin{align}
\sqrt{N}\left(\hat{\boldsymbol{\theta}}^{b}_{IVMG}-\boldsymbol{\theta}  \right)\overset{d}{\to} N\left(\boldsymbol{0},\boldsymbol{\Sigma}_{IV,\lambda} \right).
\end{align}








\section{Construction optimal instruments}
How to select optimal instruments is important issue in empirical studies.
\citet{Stephen:2001}, \citet{Kuersteiner:2010}, \citet{Ryo:2011}, \citet{Kang:2019} and \citet{Seojeong:2019} propose different methods to construct optimal instruments.
Here, we try to apply model average approach by \citet{Kuersteiner:2010} in here.  


If we can observed long past periods variables, we have many instruments. In this case we define our instruments, as
\begin{align}
\boldsymbol{Z}^{(j)}_{i}=\left( \boldsymbol{x}_{i,\cdot}, \boldsymbol{x}_{i,-1},\ldots, \boldsymbol{x}_{i,-j} \right)^{'},
\end{align}
where $\boldsymbol{Z}^{(j)}_{i}$ is $T \times  (j+1)k$ matrix with $j \geq 1$. \\




Then, we can define IV estimator, $\boldsymbol{\theta}^{b}_{i}$, as
\begin{align}
\hat{\boldsymbol{\theta}}^{b}_{IV,i}= \left( \tilde{\boldsymbol{W}}^{'}_{i}\boldsymbol{P}^{(j)}_{i} \tilde{\boldsymbol{W}}_{i} \right)^{-1} \tilde{\boldsymbol{W}}^{'}_{i}\boldsymbol{P}^{(j)}_{i} \tilde{\boldsymbol{y}}_{i},
\end{align}
where $\boldsymbol{P}^{(j)}_{i}=\boldsymbol{Z}^{(j)}_{i} \left(\boldsymbol{Z}^{(j)'}_{i}\boldsymbol{Z}^{(j)}_{i} \right)^{-1}\boldsymbol{Z}^{(j)'}_{i}$ and $b=1$ corresponds to the fixed effects model while $b=2$ corresponds to the trend models. 

If we can observe long lagged length from data, we have many instruments that can be used.
In empirical study, researchers do not use all past variables as instruments because there are trade off between efficiency and bias.
But we have not clearly know that the effect of using long lagged length  IV estimator in heterogeneous dynamic panel data model. And how to select the instruments to balance the bias and efficiency.

\citet{Kuersteiner:2010} provided model averaging two stage least squares estimator to balance the bias and efficiency. We try to follow this method to construct the optimal instruments. We define a weighting vector $\boldsymbol{H}=\left(h_{1}, \ldots, h_{j} \right)^{'}$. Then, we can weight $\boldsymbol{P}^{(j)}_{i}$ as
\begin{align}
\boldsymbol{P}\left(\boldsymbol{H} \right)= \sum^{J}_{j=1}h_{j} \boldsymbol{P}^{(j)}_{i},
\end{align}
where $J$ is maximum number of lagged variables that we can observed. 
Our goal is estimating $\boldsymbol{H}$ to minimize the approximate mean square error. 










\section{Estimation method on dynamic heterogeneous panel data model with  multifactor error structure}
For convenient, we assume the number of regressor is $1$ and we express the model as
\begin{align}
y_{i,t}=\phi_{i} y_{i,t-1}+ \sum^{k}_{\ell=1}\beta_{\ell i}x_{\ell i,t}+u_{i,t}, \,\, for\,\,i=1,\ldots N;\,t=1,\ldots,T, \, \ell=1,\ldots, k. \label{46}
\end{align}
Consider the model $(\ref{45})$, we drawn $x_{\ell i,t}$ as
\begin{align}
x_{\ell i,t}=\boldsymbol{\gamma}^{0'}_{xi}\boldsymbol{f}^{0}_{xt}+\varepsilon_{xi,t}
\end{align}
and the idiosyncratic errors of the process for $y_{i,t}$ as
\begin{align}
u_{i,t}=\boldsymbol{\gamma}^{0'}_{yi}\boldsymbol{f}^{0}_{yt}+\varepsilon_{yi,t},
\end{align}
where $\boldsymbol{\gamma}^{0}_{yi}$ and $\boldsymbol{\gamma}^{0}_{xi}$ are $m_{y}\times 1$ and $m_{x}\times 1$ true
 factor loading respectively, $\boldsymbol{f}^{0}_{yt}$  and  $\boldsymbol{f}^{0}_{xt}$ are  $m_{y}\times 1$ and $m_{x}\times 1$ true vector of unobservable factors respectively.
\subsection{Norkutes' (2019) IVMG estimator}
 We asymptotically eliminate the common factor in $\boldsymbol{x}_{i}$ by projecting matrix, $\boldsymbol{M}_{F^{0}_{x}}$.
 \begin{align}
 \boldsymbol{M}_{F^{0}_{x}}=\boldsymbol{I}_{T}-\boldsymbol{F}^{0}_{x}\left(\boldsymbol{F}^{0'}_{x}\boldsymbol{F}^{0}_{x}  \right)^{-1}\boldsymbol{F}^{0'}_{x} ; \boldsymbol{M}_{F^{0}_{x,-1}}=\boldsymbol{I}_{T}-\boldsymbol{F}^{0}_{x,-1}\left(\boldsymbol{F}^{0'}_{x,-1}\boldsymbol{F}^{0}_{x,-1}  \right)^{-1}\boldsymbol{F}^{0'}_{x,-1}
 \end{align}
And using the defactored covariates as instruments, as
\begin{align}
\boldsymbol{Z}_{IVi}=\left(\boldsymbol{M}_{F^{0}_{X}}\boldsymbol{x}_{i}, \boldsymbol{M}_{F^{0}_{x,-1}}\boldsymbol{X}_{i,-1}  \right)
\end{align}

The first step IV estimator can be expressed as
\begin{align}
\begin{split}
\boldsymbol{\hat{\varphi}}_{IVi}&=\left(\left(\frac{\boldsymbol{Z}^{'}_{i}\boldsymbol{M}_{F^{0}_{X}}\boldsymbol{W}_{i}}{T } \right)^{'} \left(\frac{\boldsymbol{Z}^{'}_{i}\boldsymbol{M}_{F^{0}_{X}}\boldsymbol{Z}_{i}}{T}  \right)^{-1}  \left(\frac{\boldsymbol{Z}^{'}_{i}\boldsymbol{M}_{F^{0}_{X}}\boldsymbol{W}_{i}}{T}  \right)\right)^{-1} \\
&\left(\left( \frac{\boldsymbol{Z}^{'}_{i}\boldsymbol{M}_{F^{0}_{X}}\boldsymbol{W}_{i}}{T}  \right)^{'} \left(\frac{\boldsymbol{Z}^{'}_{i}\boldsymbol{M}_{F^{0}_{X}}\boldsymbol{Z}_{i}}{T}  \right)^{-1}  \left(\frac{\boldsymbol{Z}^{'}_{i}\boldsymbol{M}_{F^{0}_{X}}\boldsymbol{y}_{i}}{T}  \right)\right).
\end{split}
\end{align}







\section{Monte Carlo simulation design}
\subsection{dynamic heterogeneous panels data model without error factor structure}
The data generating process:
\begin{align}
\begin{split}
y_{i,t}&= \phi_{i} y_{i,t-1}+ \sum^{k}_{\ell=1}\beta_{\ell i}x_{\ell i,t}+u_{i,t}, \,\, for\,\,i=1,\ldots N;\,t=-49,\ldots,T\, , \\ \label{M1}
x_{\ell i,t}&=\sum^{k}_{\ell=1}\phi_{\ell i}x_{\ell i,t-1}+v_{\ell i,t},
\end{split}
\end{align}
where $u_{i,t}\overset{i.i.d.}{\sim} \mathcal{N}(0,\,1)\,,$ and $v_{\ell i,t}=\rho_{v, \ell}v_{\ell i,t-1}+\left( 1-\rho^{2}_{v, \ell} \right)^{\frac{1}{2}}\varpi_{\ell i,t}, \varpi_{\ell i,t} \overset{i.i.d.}{\sim} U(0.5, 1.5) \, ,\rho_{v, \ell}=0.5.$

The slope coefficients are generated as
\begin{align}
\phi_{i}=\phi+\eta_{\phi i},\,\, \beta_{1,i}=\beta_{1}+\eta_{\beta_{1} i}\, and\, \beta_{2,i}=\beta_{2}+\eta_{\beta_{2}i}.
\end{align}
Here we consider $\phi \in \left\{0.5 \right\}$, $\beta_{1}=3$ and $\beta_{2}=1$. For the design of heterogenous slopes, $\eta_{\phi i} \overset{i.i.d.}{\sim} U\left( -c, c\right)$, and
\begin{align}
\eta_{\beta_{\ell}i}= \left(1-\rho^{2}_{\beta}  \right)^{1/2}\eta_{\phi i}.
\end{align}
Here, we set $c=0.2,\, \rho_{\beta}=0.4$ for $\ell=1,2.$


\subsection{Dynamic heterogeneous panels data model with multi-factor error structure}
This Monte Carlo simulation design same as \citet{Norkute:2019}.
For convenience, we rewrite the data generating process as bellow
\begin{align}
y_{i,t}=\alpha_{i}+\phi_{i} y_{i,t-1}+ \sum^{k}_{\ell=1}\beta_{\ell i}x_{\ell i,t}+u_{i,t}, \,\, for\,\,i=1,\ldots N;\,t=-49,\ldots,T\, . \\ \label{M1}
\end{align}
We allow error factor structure in the model as
\begin{align}
u_{i,t}=    \sum^{m_{y}}_{s=1}\gamma^{0}_{si}f^{0}_{s,t} +\varepsilon_{i,t},
\end{align}
where
\begin{align}
f^{0}_{s,t}=\rho^{0}_{s,t} f^{0}_{s,t-1}+\left( 1-\rho^{2}_{fs} \right)^{1/2}\zeta_{s,t},
\end{align}
with $\zeta_{s,t} \overset{i.i.d.}{\sim} N(0,1)$ for $s=1,\ldots m_{y}$. We assume $k=2$ and $m_{y}=1+k=3$ and set $\rho^{0}_{s,t}=0.5$ for all $s$. The error term, $\varepsilon_{i,t}$, setting as
\begin{align}
\varepsilon_{i,t}=\varsigma_{\varepsilon}\sigma_{it}\left(\epsilon_{it}-1 \right)/\sqrt{2},
\end{align}
where $ \epsilon_{it}\overset{i.i.d.}{\sim} \chi^{2}_{1},\, \sigma^{2}_{it}=\eta_{i}\varphi_{t}, \,\eta_{i}\overset{i.i.d.}{\sim}\chi^{2}_{2}/2,$  and  $\varphi_{t}=t/T\, for \,\, t=0,\ldots, T.$ And we set
\begin{align}
\varsigma_{\varepsilon}=\frac{\pi_{\mu}}{1-\pi_{\mu}}m_{y}.
\end{align}
we set $\pi_{\mu} \in \left\{ 3/4\right\}.$

The process of regressors is
\begin{align}
x_{\ell it}=\mu_{\ell i}++\sum^{k}_{\ell=1}\phi_{\ell i}x_{\ell i,t-1}+\sum^{m_{x}}_{s=1}\gamma^{0}_{\ell si}f^{0}_{s,t}+v_{\ell it}, \,\, for\,\,i=1,\ldots N;\,t=-49,\ldots,T\,; \, \ell=1,2.
\end{align}
We set number of factor, $m_{x}$, is $2$. Therefore, $\boldsymbol{f}^{0}_{y,t}=\left(f^{0}_{1t}, f^{0}_{2t},f^{0}_{3t}  \right)^{'}$ and $\boldsymbol{f}^{0}_{x,t}=\left(f^{0}_{1t}, f^{0}_{2t}  \right)^{'}.$
We set
\begin{align}
v_{\ell i,t}=\rho_{v, \ell}v_{\ell i,t-1}+\left( 1-\rho^{2}_{v, \ell} \right)^{\frac{1}{2}}\varpi_{\ell i,t},  for \, \ell=1,2,
\end{align}
  where $\rho_{v, \ell}=0.5$ for all $\ell$.
The individual effect is
\begin{align}
 \alpha^{\ast}_{i}\overset{i.i.d.}{\sim} N\left(0, (1-\rho_{i})^{2} \right), \,\, \mu^{\ast}_{\ell i}=\rho_{\mu,\ell}\alpha^{\ast}_{i}+\left( 1-\rho^{2}_{\mu,\ell} \right)^{1/2}\omega_{\ell i},
\end{align}
where $\omega \overset{i.i.d.}{\sim}N\left(0, (1-\rho_{i})^{2} \right)$ and $\rho_{\mu,\ell}=0.5$.

Now, we define the factor loading in $u_{i,t}$ are generated as $\gamma^{0\ast}_{si}\overset{i.i.d.}{\sim}N\left(0, 1 \right)$, for $s=1,\ldots, m_{y}=3$, and the factor loading in $x_{1it}$ and $x_{2it}$ are drawn as
\begin{align}
\begin{split}
\gamma^{0\ast}_{1si}&=\rho_{\gamma,1s} \gamma^{0\ast}_{3i}+\left(1-\rho^{2}_{\gamma,1s}  \right)^{1/2}\xi_{1si};\,\xi_{1si}\overset{i.i.d.}{\sim}N\left(0, 1 \right); \\
\gamma^{0\ast}_{2si}&=\rho_{\gamma,2s} \gamma^{0\ast}_{si}+\left(1-\rho^{2}_{\gamma,2s}  \right)^{1/2}\xi_{2si};\,\xi_{2si}\overset{i.i.d.}{\sim}N\left(0, 1 \right);
\end{split}
\end{align}
for $s=1,\ldots, m_{x}=2$. We set $\rho_{\gamma, 11}=\rho_{\gamma, 12} \in \left\{ 0.5 \right\}$ and $\rho_{\gamma, 21}=\rho_{\gamma, 22}=0.5.$
The factor loading are generated as
\begin{align}
\boldsymbol{\Gamma}=\boldsymbol{\Gamma}^{0}+\boldsymbol{\Gamma}^{0\ast}_{i}
\end{align}
where
\begin{align}
\boldsymbol{\Gamma}^{0}_{i}=
\begin{bmatrix}
\gamma^{0}_{1i} & \gamma^{0}_{11i} & \gamma^{0}_{21i} \\
\gamma^{0}_{2i} & \gamma^{0}_{12i} & \gamma^{0}_{22i} \\
\gamma^{0}_{3i} &  0               &     0
\end{bmatrix}
\end{align}
and
\begin{align}
\boldsymbol{\Gamma}^{0\ast}_{i}=
\begin{bmatrix}
\gamma^{0\ast}_{1i} & \gamma^{0\ast}_{11i} & \gamma^{0\ast}_{21i} \\
\gamma^{0\ast}_{2i} & \gamma^{0\ast}_{12i} & \gamma^{0\ast}_{22i} \\
\gamma^{0\ast}_{3i} &  0               &     0
\end{bmatrix}.
\end{align}
We set
\begin{align}
\boldsymbol{\Gamma}^{0}=
\begin{bmatrix}
1/4 & 1/4 & -1 \\
1/2 & -1  & 1/4 \\
1/2 & 0   & 0
\end{bmatrix} .
\end{align}
And
\begin{align}
\alpha_{i}=\alpha+ \alpha^{\ast}_{i}, \, \mu_{\ell i}= \mu_{\ell}+\mu^{\ast}_{\ell i},
\end{align}
where $\alpha=1/2$, $\mu_{1}=1$, $\mu_{2}=-1/2$.

The slope coefficients are generated as
\begin{align}
\phi_{i}=\phi+\eta_{\phi i},\,\, \beta_{1,i}=\beta_{1}+\eta_{\beta_{1} i}\, and\, \beta_{2,i}=\beta_{2}+\eta_{\beta_{2}i}.
\end{align}
Here we consider $\phi \in \left\{0.5\right\}$, $\beta_{1}=3$ and $\beta_{2}=1$. For the design of heterogenous slopes, $\eta_{\phi i} \overset{i.i.d.}{\sim} U\left( -c, c\right)$, and
\begin{align}
\eta_{\beta_{\ell}i}=\left[(2c)^{2}/12 \right]\rho_{\beta}\xi_{\beta \ell i}+ \left(1-\rho^{2}_{\beta}  \right)^{1/2}\eta_{\phi i},
\end{align}
where
\begin{align}
\xi_{\beta \ell i}=\frac{\bar{v^{2}_{\ell i}}- \bar{v^{2}_{\ell}}}{\left[ N^{-1}\sum^{N}_{i=1} \left( \bar{v^{2}_{\ell i}}- \bar{v^{2}_{\ell }}\right)^{2} \right]^{1/2} },
\end{align}
with $\bar{v^{2}_{ell i}}=T^{-1}\sum^{T}_{t=1}v^{2}_{\ell i t}$, $\bar{v^{2}_{\ell}}=N^{-1} \sum^{N}_{i=1} \bar{v^{2}_{\ell i}}$, for $\ell=1,2.$
Here, we set $c=0.2,\, \rho_{\beta}=0.4$ for $\ell=1,2.$ And
\begin{align}
\varsigma^{2}_{v}=\varsigma^{2}_{\varepsilon}\left[SNR-\frac{\rho^{2}_{v}}{1-\rho^{2}_{v}}   \right]\left(\frac{\beta^{2}_{1}+\beta^{2}_{2}}{1-\rho^{2}_{v}}  \right)^{-1},
\end{align}

where $SNR=4$. For the $(T,N)$, we consider $T \in \left\{25, 50, 100, 200  \right\}$ and  $N \in \left\{25, 50, 100, 200  \right\}.$




\section{Monte Carlo simulation results}
\subsection{Dynamic Heterogeneous Panels without multifactor error structure }
We consider ARDL(1,0) model. \\
$\phi \in \left\{0.5 \right\}.$ \\
 $\beta_{1}=3.$ \\
  $\beta_{2}=1.$ \\
$u_{i,t}\overset{i.i.d.}{\sim} \mathcal{N}(0,\,1).$ \\
$\varpi_{\ell i,t} \overset{i.i.d.}{\sim} U(0.5, 1.5).$ \\
$\rho_{v, \ell}=0.5.$ \\
  $c=0.2.$ \\
  $ \rho_{\beta}=0.4.$ \\
 $T \in \left\{25, 50, 100, 200  \right\}.$ \\
 $N \in \left\{25, 50, 100, 200  \right\}.$ \\

LSMG estimator is provided in sheet 1 of MC.xlsx file. \\
IVMG estimator is provided in sheet 2 of MC.xlsx file. \\




\subsection{Dynamic Heterogeneous Panels with multifactor error structure }
We consider ARDL(1,0) model. \\
$\phi \in \left\{0.5 \right\}.$ \\
 $\beta_{1}=3.$ \\
  $\beta_{2}=1.$ \\
  $k=2.$ \\
  $m_{y}=1+k=3.$\\
  $m_{x}=k=2.$\\
$\zeta_{s,t} \overset{i.i.d.}{\sim} N(0,1)$ \\
$\pi_{\mu} \in \left\{ 3/4\right\}.$ \\
$\rho^{0}_{s,t}=0.5.$ \\
$\rho_{v, \ell}=0.5.$\\
 $\rho_{\mu,\ell}=0.5.$ \\
  $\gamma^{0\ast}_{si}\overset{i.i.d.}{\sim}N\left(0, 1 \right).$ \\
 $\xi_{1si}\overset{i.i.d.}{\sim}N\left(0, 1 \right).$ \\
$\xi_{2si}\overset{i.i.d.}{\sim}N\left(0, 1 \right).$ \\
$\rho_{\gamma, 11}=\rho_{\gamma, 12} \in \left\{ 0.5 \right\}.$ \\
$\rho_{\gamma, 21}=\rho_{\gamma, 22}=0.5.$ \\
$
\boldsymbol{\Gamma}^{0}=
\begin{bmatrix}
1/4 & 1/4 & -1 \\
1/2 & -1  & 1/4 \\
1/2 & 0   & 0
\end{bmatrix} .
$ \\
$\alpha=1/2.$ \\
 $\mu_{1}=1.$\\
  $\mu_{2}=-1/2.$ \\
$c=0.2.$ \\
$\rho_{\beta}=0.4.$ \\
$SNR=4.$ \\
$T \in \left\{25, 50, 100, 200  \right\}.$ \\
 $N \in \left\{25, 50, 100, 200  \right\}.$ \\

IVMG estimator is provided in sheet 3 of MC.xlsx file. \\

\section{Short summary}
\subsection{Dynamic Heterogeneous Panels without multifactor error structure}
1. The performance of IVMG estimator is better than LSMG estimator in bias and RMSE.


\subsection{Dynamic Heterogeneous Panels with multifactor error structure}
1. When $N$ and $T$ increase, the performance of IVMG estimator is good in bias and RMSE. \\
Related literature to dynamic Heterogeneous Panels with multifactor error structure: \citet{Chudik:2015} and \citet{Norkute:2019}.\\
Related literature to choosing number of instruments: \citet{Stephen:2001}, \citet{Swanson:2005}, \citet{Marine:2012}, \citet{Bai:2010} and \citet{Kang:2019}.


\newpage
\appendix
\appendixpage
\section{Asymptotic property of LS estimator}


Based on heterogenous dynamic panel data model $(\ref{1})$, we can obtain fixed effect estimator as
\begin{align}
\hat{\boldsymbol{\theta}}_{LS,i}=
\begin{pmatrix}
\hat{\phi}_{i} \\
\hat{\beta}_{i}
\end{pmatrix}=
\begin{pmatrix}
\frac{\sum^{T}_{t=1}\tilde{y}^{2}_{i,t-1}}{T} & \frac{\sum^{T}_{t=1}\tilde{y}_{i,t-1}\tilde{x}_{i,t}}{T} \\
\frac{\sum^{T}_{t=1}\tilde{x}_{i,t}\tilde{y}_{i,t-1}}{T} &  \frac{\sum^{T}_{t=1}\tilde{x}_{i,t}^{2}}{T}
\end{pmatrix}^{-1}
\begin{pmatrix}
\frac{\sum^{T}_{t=1}\tilde{y}_{i,t-1}\tilde{y}_{i,t}}{T} \\
\frac{\sum^{T}_{t=1}\tilde{x}_{i,t}\tilde{y}_{i,t}}{T}
\end{pmatrix},
\end{align}

where $\tilde{y}_{i,t}=y_{i,t}-\bar{y}_{i}$, $\tilde{y}_{i,t-1}=y_{i,t-1}-\bar{y}_{i,-1}\iota_{T}$ and $\tilde{x}_{i,t}=x_{i,t}-\bar{x}_{i}$ with $\bar{y}_{i}=\frac{1}{T}\sum^{T}_{t=1}y_{i,t}$, $\bar{y}_{i,-1}=\frac{1}{T}\sum^{T}_{t=1}y_{i,t-1}$,  $\bar{x}_{i}=\frac{1}{T}\sum^{T}_{t=1}x_{i,t}$.  Under equation $(\ref{1})$, we have
\begin{align}
\begin{pmatrix}
\hat{\phi}_{i}-\phi_{i} \\
\hat{\beta}_{i}-\beta_{i}
\end{pmatrix}=
\begin{pmatrix}
\frac{\sum^{T}_{t=1}\tilde{y}^{2}_{i,t-1}}{T} & \frac{\sum^{T}_{t=1}\tilde{y}_{i,t-1}\tilde{x}_{i,t}}{T} \\
\frac{\sum^{T}_{t=1}\tilde{x}_{i,t}\tilde{y}_{i,t-1}}{T} &  \frac{\sum^{T}_{t=1}\tilde{x}_{i,t}^{2}}{T}
\end{pmatrix}^{-1}
\begin{pmatrix}
\frac{\sum^{T}_{t=1}\tilde{y}_{i,t-1}\tilde{u}_{i,t}}{T} \\
\frac{\sum^{T}_{t=1}\tilde{x}_{i,t}\tilde{u}_{i,t}}{T}
\end{pmatrix},
\end{align}
where $\tilde{u}_{i,t}$ is $u_{i,t}-\bar{u}_{i}$ with $\bar{u}_{i}=\frac{1}{T}\sum^{T}_{t=1}u_{i,t}.$


Now, we can investigate asymptotic bias by taking the probability limit as
\begin{align}
A^{(1)}_{\phi i}=\plim_{T  \rightarrow \infty}\left( \frac{\sum^{T}_{t=1}\tilde{y}_{i,t-1}\tilde{u}_{i,t}}{T} \right). \label{11}
\end{align}
Then $A^{(1)}_{i}$ can be taken expectations as
\begin{align}
\begin{split}
A^{(1)}_{\phi i}&=E\left(y_{i,t-1}-\bar{y}_{i,-1} \right) \left(u_{i,t}-\bar{u}_{i} \right) \\
&= E\left( y_{i,t-1}u_{i,t}\right) - E\left( y_{i,t-1} \bar{u}_{i}  \right)-E\left(\bar{y}_{i,-1}   u_{i,t}\right)+E\left(\bar{y}_{i,-1}  \bar{u}_{i}\right),
\end{split}
\end{align}
where $E\left( y_{i,t-1}u_{i,t}\right)=0$.

And we assume $y_{i,t}$ has started from a long time period in the past, so we have
\begin{align}
y_{i,t}=\frac{\alpha_{i}}{\left(1-\phi_{i} \right)}+\sum^{\infty}_{s=0}\beta_{i}\phi^{s}_{i}x_{i,t -s}+\sum^{\infty}_{s=0}\phi^{s}_{i}u_{i,t-s},
\end{align}

Then, we have
\begin{align}
\begin{split}
A^{(1)}_{\phi i}&=-E\left( \left(\sum^{\infty}_{s=0}\phi^{s}_{i}u_{i,t-s-1} \right) \left(\frac{1}{T}\sum^{T}_{t=1}u_{i,t} \right)  \right)-E\left(\frac{u_{i,t}}{T}\sum^{T}_{t=1}\sum^{\infty}_{s=0}\phi^{s}_{i}u_{i,t-s-1} \right)+ \\
&\left(\frac{1}{T}\sum^{T}_{t=1}\sum^{\infty}_{s=0}\phi^{s}_{i}u_{i,t-s-1} \right)\left( \frac{1}{T}\sum^{T}_{t=1}u_{i,t} \right). \label{12}
\end{split}
\end{align}
 Hence, from above equation, we have
 \begin{equation}
\begin{split}
A^{(1)}_{\phi i}=&-\dfrac{1}{T} E\left\{  \left( u_{i,t-1}+u_{i,t-2}\phi_{i}^{1}+u_{i,t-3}\phi_{i}^{2}+\ldots\right)\left(u_{i,1}+\ldots+u_{i,t-1}+u_{i,t}+\ldots+u_{i,T}\right) \right\} -\\
 &\dfrac{1}{T} E \left\{  u_{i,t}\sum^{T}_{s=1}\left( u_{i,s-1}\phi_{i}^{0}+u_{i,s-2}\phi_{i}^{1}+\ldots+u_{i,s-t-1}\phi_{i}^{t}+\ldots+u_{i,s-T-1}\phi_{i}^{T}+\ldots \right)  \right\}  + \\
 +& \dfrac{1}{T} E\{ \left( \sum^{T}_{s=1}u_{i,s-1}\phi_{i}^{0}+\sum^{T}_{s=1}u_{i,s-2}\phi_{i}^{1}+\ldots+\sum^{T}_{s=1}u_{i,s-t-1}\phi_{i}^{t}+\ldots
+\sum^{T}_{s=1}u_{is-T-1}\phi_{i}^{T}+\ldots \right)\\
 &\left(\dfrac{1}{T}\sum^{T}_{s=1}u_{i,s}\right) \} \\
 =&-\dfrac{\sigma_{u}^{2}}{T}\dfrac{(1-\phi_{i}^{t-1})}{1-\phi_{i}}-\dfrac{\sigma^{2}_{u}}{T}\dfrac{(1-\phi_{i}^{T-t})}{(1-\phi_{i})}+\dfrac{\sigma^{2}_{u}}{T} \left( \dfrac{1}{1-\phi_{i}}-\dfrac{1}{T}\dfrac{(1-\phi_{i}^{T})}{(1-\phi_{i})^{2}}\right) \\
 =&-\dfrac{\sigma_{u}^{2}}{T(1-\phi_{i})}\left( 1-\phi_{i}^{t-1}-\phi_{i}^{T-t}+\dfrac{1}{T}\dfrac{(1-\phi_{i}^{T})}{(1-\phi_{i})}  \right).
 \end{split}
\end{equation}


Therefore, we can see the bias of $\hat{\phi}_{i}$ is $O\left(T^{-1}\right)$.

To be more compact ,we can rewrite the model as,
\begin{align}
\tilde{\boldsymbol{y}}_{i}=\tilde{\boldsymbol{W}}_{i}\boldsymbol{\theta}_{i}+\tilde{\boldsymbol{u}}_{i},
\end{align}
where $\tilde{\boldsymbol{y}}_{i}=\left(\tilde{\boldsymbol{y}}_{i,1},\ldots, \tilde{\boldsymbol{y}}_{i,T}  \right)^{'}$ is $T \times 1$ vector,  $\tilde{\boldsymbol{W}}_{i}=\left( \tilde{\boldsymbol{w}}_{i,1},\ldots, \tilde{\boldsymbol{w}}_{i,T} \right)^{'}$ is $T \times 2$ matrix and $\tilde{\boldsymbol{u}}_{i}=\left(\tilde{\boldsymbol{u}}_{i,1}, \ldots, \tilde{\boldsymbol{u}}_{i,T} \right)$ is $T \times 1$ vector with $\tilde{\boldsymbol{w}}_{i,t}=\left(y_{i,t-1}-\bar{y}_{i,-1}, x_{i,t}-\bar{x}_{i}  \right)^{'}$, for $t=1,\ldots, T$ and $i=1,\ldots, N.$

Also, we define our interested parameter as
\begin{align}
\left(\phi_{i},\beta_{i}  \right)^{'}=\boldsymbol{\theta}_{i}=\boldsymbol{\theta}+\boldsymbol{\lambda}_{i},
\end{align}
where $\boldsymbol{\lambda}_{i}\overset{i.i.d.}{\sim} \left(\boldsymbol{0}, \boldsymbol{\Sigma}_{\lambda} \right).$
The lest square estimator, $\hat{\boldsymbol{\theta}}_{LS,i}$, can be expressed as
\begin{align}
\hat{\boldsymbol{\theta}}_{LS,i}=\left(\frac{\tilde{\boldsymbol{W}}^{'}_{i} \tilde{\boldsymbol{W}}_{i}}{T}  \right)^{-1} \frac{ \tilde{\boldsymbol{W}}^{'}_{i}\tilde{\boldsymbol{y}}_{i}}{T}. \label{13}
\end{align}

From above discussion and assumptions, we have following theorem \\
\begin{myTheo}
\begin{align}
\sqrt{T}\left(\hat{\boldsymbol{\theta}}_{IV,i}-\boldsymbol{\theta}_{i}\right) \overset{d}{\to} N \left(\boldsymbol{0}, \boldsymbol{Q}_{i}^{-1}\boldsymbol{\Sigma}_{LS,i}\boldsymbol{Q}_{i}^{-1}  \right),
\end{align}
where $\boldsymbol{\Sigma}_{LS,i}=\plim_{T \to \infty} T^{-1}\tilde{\boldsymbol{W}}^{'}_{i} \tilde{\boldsymbol{u}}_{i} \tilde{\boldsymbol{u}}^{'}_{i}  \tilde{\boldsymbol{W}}_{i} $ and $\boldsymbol{Q}_{i}=\plim_{T \to \infty} T^{-1}\tilde{\boldsymbol{W}}^{'}_{i}\tilde{\boldsymbol{W}}_{i}$
\end{myTheo}



\subsubsection{Mean group LS estimator }
Now, we define the mean group estimator of $\boldsymbol{\theta}$:
\begin{align}
\hat{\boldsymbol{\theta}}_{LSMG}=\frac{1}{N}\sum^{N}_{i=1}\hat{\boldsymbol{\theta}}_{LSi}.
\end{align}
And we can show that the asymptotic property of $\hat{\boldsymbol{\theta}}_{LSMG}$, as
\begin{align}
\begin{split}
\hat{\boldsymbol{\theta}}_{LSMG}&=N^{-1}\sum^{N}_{i=1}\left(\frac{\tilde{\boldsymbol{W}}^{'}_{i} \tilde{\boldsymbol{W}}_{i}}{T}  \right)^{-1} \frac{ \tilde{\boldsymbol{W}}^{'}_{i}\tilde{\boldsymbol{y}}_{i}}{T} \\
&=\bar{\boldsymbol{\theta}}+N^{-1}\sum^{N}_{i=1}\left(\frac{\tilde{\boldsymbol{W}}^{'}_{i} \tilde{\boldsymbol{W}}_{i}}{T}  \right)^{-1} \frac{ \tilde{\boldsymbol{W}}^{'}_{i}\tilde{\boldsymbol{u}}_{i}}{T},
\end{split}
\end{align}
where $\bar{\boldsymbol{\theta}}=N^{-1}\sum^{N}_{i=1}\boldsymbol{\theta}_{i}.$
For fixed $N$ and large $T$, we have
\begin{align}
\plim_{T \to \infty}\hat{\boldsymbol{\theta}}_{LSMG} =  \bar{\boldsymbol{\theta}}+ N^{-1}\sum^{N}_{i=1}\plim_{T \to \infty}\left(\frac{\tilde{\boldsymbol{W}}^{'}_{i} \tilde{\boldsymbol{W}}_{i}}{T}  \right)^{-1}\plim_{T \to \infty}\left( \frac{ \tilde{\boldsymbol{W}}^{'}_{i}\tilde{\boldsymbol{u}}_{i}}{T}\right)
\end{align}
 Then, from section $1.1$, we know that $\plim_{T \to \infty}\left( \frac{ \tilde{\boldsymbol{W}}^{'}_{i}\tilde{\boldsymbol{u}}_{i}}{T}\right)=O_{p}(1)$. Thus,  we can obtain
\begin{align}
\plim_{T \to \infty}\hat{\boldsymbol{\theta}}_{LSMG} =  \bar{\boldsymbol{\theta}}.
\end{align}
When $N \to \infty$ and $T \to \infty$ and by the law of large numbers, we can see that
\begin{align}
\plim_{T \to \infty, N \to \infty}\hat{\boldsymbol{\theta}}_{LSMG}=\boldsymbol{\theta}.
\end{align}

And the variance estimator of $\hat{\boldsymbol{\theta}}_{LSMG}$ is given by
\begin{align}
\hat{\boldsymbol{\Sigma}}_{LS,\lambda}=\frac{1}{N-1}\sum^{N}_{i=1}\left( \hat{\boldsymbol{\theta}}_{LS,i}- \hat{\boldsymbol{\theta}}_{LSMG}\right)\left( \hat{\boldsymbol{\theta}}_{LS,i}- \hat{\boldsymbol{\theta}}_{LSMG}\right)^{'}.
\end{align}

We show that $\hat{\boldsymbol{\Sigma}}_{LS,\lambda}$ is consistent when $N \to \infty$ and $T \to \infty.$
\begin{align}
\begin{split}
&\sum^{N}_{i=1}\left( \hat{\boldsymbol{\theta}}_{LS,i}- \hat{\boldsymbol{\theta}}_{LSMG}\right)\left( \hat{\boldsymbol{\theta}}_{LS,i}- \hat{\boldsymbol{\theta}}_{LSMG}\right)^{'}=\sum^{N}_{i=1}\left( \hat{\boldsymbol{\theta}}_{LS,i}- E\left(\hat{\boldsymbol{\theta}}_{LS, i}\right) \right)\left( \hat{\boldsymbol{\theta}}_{LS,i}- E\left(\hat{\boldsymbol{\theta}}_{LS, i}\right)\right)^{'}\\
&+\sum^{N}_{i=1}\left( \hat{\boldsymbol{\theta}}_{LSMG}- E\left(\hat{\boldsymbol{\theta}}_{LS, i}\right)\right) \left( \hat{\boldsymbol{\theta}}_{LSMG}- E\left(\hat{\boldsymbol{\theta}}_{LS, i}\right)\right)^{'}\\
&-\sum^{N}_{i=1}\left( \hat{\boldsymbol{\theta}}_{LSMG}- E\left(\hat{\boldsymbol{\theta}}_{LS, i}\right)\right) \left( \hat{\boldsymbol{\theta}}_{LS,i}- E\left(\hat{\boldsymbol{\theta}}_{LS, i}\right)\right)^{'} \\
&-\sum^{N}_{i=1}\left( \hat{\boldsymbol{\theta}}_{LS,i}- E\left(\hat{\boldsymbol{\theta}}_{LS, i}\right)\right) \left( \hat{\boldsymbol{\theta}}_{LSMG}- E\left(\hat{\boldsymbol{\theta}}_{LS, i}\right)\right)^{'}. \label{20}
\end{split}
\end{align}

Taking expectation on equation $(\ref{20})$, we have
\begin{align}
\begin{split}
&E\left( \sum^{N}_{i=1}\left( \hat{\boldsymbol{\theta}}_{LS,i}- \hat{\boldsymbol{\theta}}_{LSMG}\right)\left( \hat{\boldsymbol{\theta}}_{LS,i}- \hat{\boldsymbol{\theta}}_{LSMG}\right)^{'} \right)=
 \sum^{N}_{i=1}Var\left(\hat{\boldsymbol{\theta}}_{LS,i}\right)+
\sum^{N}_{i=1}\hat{\boldsymbol{\theta}}_{LSMG}\hat{\boldsymbol{\theta}}^{'}_{LSMG}+  \\
&\sum^{N}_{i=1}E\left(\hat{\boldsymbol{\theta}}_{LS,i}\right) E\left(\hat{\boldsymbol{\theta}}_{LS,i}\right)^{'}-
\sum^{N}_{i=1}\hat{\boldsymbol{\theta}}_{LSMG}\hat{\boldsymbol{\theta}}^{'}_{LS,i}+
\sum^{N}_{i=1}E\left(\hat{\boldsymbol{\theta}}_{LS,i}\right)\hat{\boldsymbol{\theta}}^{'}_{LS,i}-
\sum^{N}_{i=1}\hat{\boldsymbol{\theta}}_{LSMG}E\left(\hat{\boldsymbol{\theta}}_{LS,i}\right)^{'}- \\
&\sum^{N}_{i=1}\hat{\boldsymbol{\theta}}_{LS,i}\hat{\boldsymbol{\theta}}^{'}_{LSMG}+
\sum^{N}_{i=1}\hat{\boldsymbol{\theta}}_{LS,i}E\left(\hat{\boldsymbol{\theta}}_{LS,i}\right)^{'}-
\sum^{N}_{i=1}E\left(\hat{\boldsymbol{\theta}}_{LS,i}\right) E\left(\hat{\boldsymbol{\theta}}_{LS,i}\right)^{'} = \\
&\sum^{N}_{i=1}Var\left(\hat{\boldsymbol{\theta}}_{LS,i}\right)-N E \left(\hat{\boldsymbol{\theta}}_{LSMG}\hat{\boldsymbol{\theta}}^{'}_{LSMG} \right)+\sum^{N}_{i=1}E\left(\hat{\boldsymbol{\theta}}_{LS,i}\right) E\left(\hat{\boldsymbol{\theta}}_{LS,i}\right)^{'}
\end{split}
\end{align}
and
\begin{align}
E \left(\hat{\boldsymbol{\theta}}_{LSMG}\hat{\boldsymbol{\theta}}^{'}_{LSMG} \right)=\frac{1}{N^{2}} \sum^{N}_{i=1}\sum^{N}_{j=1}E \left(\hat{\boldsymbol{\theta}}_{LS,i}\hat{\boldsymbol{\theta}}^{'}_{LS,i} \right).
\end{align}

And, we also have
\begin{align}
E \left(\hat{\boldsymbol{\theta}}_{LSMG}\hat{\boldsymbol{\theta}}^{'}_{LSMG} \right)=\frac{1}{N^{2}}\left( \sum^{N}_{i=1}Var\left(\hat{\boldsymbol{\theta}}_{LS,i}\right)+ \sum^{N}_{i=1}\sum^{N}_{j=1}E \left(\hat{\boldsymbol{\theta}}_{LS,i}\hat{\boldsymbol{\theta}}^{'}_{LS,i} \right) \right).
\end{align}
Then,
\begin{align}
\begin{split}
&E\left( \sum^{N}_{i=1}\left( \hat{\boldsymbol{\theta}}_{LS,i}- \hat{\boldsymbol{\theta}}_{LSMG}\right)\left( \hat{\boldsymbol{\theta}}_{LS,i}- \hat{\boldsymbol{\theta}}_{LSMG}\right)^{'} \right)= \\
&\left(1-\frac{1}{N} \right)\sum^{N}_{i=1}Var\left(\hat{\boldsymbol{\theta}}_{LS,i}\right)+ \sum^{N}_{i=1}E \left(\hat{\boldsymbol{\theta}}_{LS,i}\hat{\boldsymbol{\theta}}^{'}_{LS,i} \right)- \frac{1}{N}\sum^{N}_{i=1}\sum^{N}_{j=1}E \left(\hat{\boldsymbol{\theta}}_{LS,i}\hat{\boldsymbol{\theta}}^{'}_{LS,i} \right). \label{24}
\end{split}
\end{align}
From above equation $(\ref{24})$, we can observe the bias term as,
\begin{align}
\aleph=\sum^{N}_{i=1}E \left(\hat{\boldsymbol{\theta}}_{LS,i}\hat{\boldsymbol{\theta}}^{'}_{LS,i} \right)- \frac{1}{N}\sum^{N}_{i=1}\sum^{N}_{j=1}E \left(\hat{\boldsymbol{\theta}}_{LS,i}\hat{\boldsymbol{\theta}}^{'}_{LS,i} \right). \label{25}
\end{align}

 Taking expectation on equation $(26)$, we have
\begin{align}
E\left( \hat{\boldsymbol{\theta}}_{LS,i} \right)=\boldsymbol{\theta}+E \left(b_{i} \right),
\end{align}
From equation $(\ref{25})$, we know that
 \begin{align}
\aleph=\sum^{N}_{i=1}E \left(b_{i}\right) E\left(b^{'}_{i} \right)- \frac{1}{N}\sum^{N}_{i=1}\sum^{N}_{j=1}E \left(b_{i}\right) E\left(b^{'}_{j} \right).
 \end{align}
When $T \to \infty$, $\aleph=0$.
Therefore, we have following theorem \\

\begin{myTheo}
When $(T,N) \overset{j}{\to} \infty$ such that $N/T \to c$ with $0<c< \infty$,
\begin{enumerate}
\item \begin{align}
\sqrt{N}\left(\hat{\boldsymbol{\theta}}_{LSMG}-\boldsymbol{\theta}  \right)\overset{d}{\to} N\left(\boldsymbol{0},\boldsymbol{\Sigma}_{LS,\lambda} \right).
\end{align}
\item \begin{align}
\hat{\boldsymbol{\Sigma}}_{LS,\lambda} \overset{p}{\to}  \boldsymbol{\Sigma}_{LS,\lambda}
\end{align}
where
\begin{align}
 \hat{\boldsymbol{\Sigma}}_{LS,\lambda}=\frac{1}{N-1}\sum^{N}_{i=1}\left( \hat{\boldsymbol{\theta}}_{LS,i}- \hat{\boldsymbol{\theta}}_{LSMG}\right)\left( \hat{\boldsymbol{\theta}}_{LS,i}- \hat{\boldsymbol{\theta}}_{LSMG}\right)^{'}.
\end{align}
\end{enumerate}
\end{myTheo}



\addcontentsline{toc}{section}{Reference}
\renewcommand\refname{References}
\bibliographystyle{chicago}
\bibliography{1}




>>>>>>> 0c15682029aba834c6aa5b77f1e7ce6d65bf5d86
\end{document}  \href{*}{*} 