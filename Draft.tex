\documentclass[12pt,a4paper,hyperref]{article}
\usepackage[usenames,dvipsnames]{xcolor}
\definecolor{darkblue}{rgb}{0.0, 0.0, 0.55}
	\definecolor{ultramarine}{rgb}{0.07, 0.04, 0.56}
\usepackage{amsmath, natbib, latexsym, array, amssymb,longtable,float, graphicx, appendix,lscape,diagbox,textcomp,placeins}
\usepackage[colorlinks,
            linkcolor=ultramarine,
            anchorcolor=green,
            citecolor=darkblue
            ]{hyperref}
\usepackage[flushleft]{threeparttable}
\usepackage[top=2.7cm, left=3cm, right=3cm, bottom=2.7cm]{geometry}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}

\urlstyle{same}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{pgfplotstable}
\sisetup{
  round-mode          = places, % Rounds numbers
  round-precision     = 2, % to 2 places
}

\newenvironment{sequation}{\begin{equation}\tiny}{\end{equation}}
\DeclareMathOperator*{\plim}{plim}
\renewcommand{\floatpagefraction}{0.60}
\renewcommand{\appendixpagename}{\Large Appendix}
\setcounter{secnumdepth}{3}
\begin{document}
\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page



\HRule \\[0.4cm]
{ \huge \bfseries Dynamic Heterogeneous Panels }\\[0.4cm] % Title of your document
\HRule \\[1.5cm]


\vfill % Fill the rest of the page with whitespace

\end{titlepage}

\newpage
\tableofcontents
\newpage
\section{Asymptotic bias of LS estimator}
\subsection{Brief the source of bias}
Consider the dynamic heterogeneous panels data model:
\begin{align}
y_{i,t}=\phi_{i} y_{i,t-1}+ \beta_{1i}x_{i,t}+u_{i,t}, \,\, for\,\,i=1,\ldots N;\,t=1,\ldots,T\, , \label{1}
\end{align}

 Form above model, the model can rewritten as
 \begin{align}
 \Delta y_{i,t}=-\left( 1-\phi_{i}\right)\left(y_{i,t}- \pi_{i}x_{i,t}  \right)+u_{i,t},
 \end{align}
where $\pi_{i}=\frac{\beta_{i}}{1-\phi_{i}}$. And we defined $\theta_{i}=\left(1-\phi_{i} \right)$.
Then we suppose
\begin{align}
\begin{split}
\theta_{i}&=\theta+\eta_{i1}, \\
\pi_{i}&=\pi+\eta_{i2}.
\end{split}
\end{align}
Therefore, we know
\begin{align}
\begin{split}
\beta_{i}=\pi_{i}\theta_{i}=\left(\pi+\eta_{i2}  \right) \left( \theta+\eta_{i1}\right)=\pi\theta+\pi\eta_{i,1}+\theta\eta_{i2}+\eta_{i1}\eta_{i2}
\end{split}
\end{align}
And we defined $\eta_{i3}=\pi\eta_{i,1}+\theta\eta_{i2}+\eta_{i1}\eta_{i2} $. Then,we know $\beta_{i}=\pi \theta+\eta_{i3}$.
Therefore, from equation $(\ref{1})$, we have
\begin{align}
\begin{split}
y_{i,t}&=\phi_{i} y_{i,t-1}+ \left(  \pi\theta+\pi\eta_{i,1}+\theta\eta_{i2}+\eta_{i1}\eta_{i2}   \right)x_{i,t}+u_{i,t} \\
&=\left(1-\theta_{i}  \right)y_{i,t-1}+\beta x_{i,t}+\eta_{i,3}x_{i,t}+u_{i,t} \\
&=(1-\theta)y_{i,t-1}-\eta_{i1}y_{i,t-1}+\beta x_{i,t}+\eta_{i3} x_{i,t}+u_{i,t} \\
&=\phi y_{i,t-1}+ \beta x_{i,t}+\left(u_{i,t}-\eta_{i1} y_{i,t-1}+\eta_{i3} x_{i,t}  \right) \\
&=\phi y_{i,t-1}+ \beta x_{i,t}+v_{i,t},
\end{split}
\end{align}
where $v_{i,t}=\left(u_{i,t}-\eta_{i1} y_{i,t-1}+\eta_{i3} x_{i,t}  \right)$.
Then,we can see that  $y_{i,t-1}$ and $x_{i,t}$ are correlated with $v_{i,t}$.


\subsection{Asymptotic bias of LS estimator}
To be continue.



\section{estimation methods }
For convenient, we assume the number of regressor is $1$ and we express the model as
\begin{align}
y_{i,t}=\phi_{i} y_{i,t-1}+ \beta_{1i}x_{i,t}+u_{i,t}, \,\, for\,\,i=1,\ldots N;\,t=1,\ldots,T.
\end{align}
We stack the $T$ observations for each $i$ yield
\begin{align}
\boldsymbol{y}_{i}=\phi \boldsymbol{y}_{i,-1}+ \boldsymbol{x}_{i}\beta +\boldsymbol{u}_{i},
\end{align}
where $\boldsymbol{y}_{i}=\left(y_{i,1},  \ldots, y_{i,T} \right)^{'}$, $\boldsymbol{y}_{i,-1}=\left(y_{i,0},  \ldots, y_{i,T-1} \right)^{'}$, $\boldsymbol{x}_{i}=\left(x_{i,1},  \ldots, x_{i,T} \right)^{'}$ and $\boldsymbol{u}_{i}=\left(u_{i,1}, \ldots, u_{i,T} \right)$. To be more compressive, the model can be expressed as
\begin{align}
\boldsymbol{y}_{i}=\boldsymbol{W}_{i}\boldsymbol{\varphi}+\boldsymbol{u}_{i}, \label{7}
\end{align}
 where $\boldsymbol{W}_{i}=\left(\boldsymbol{y}_{i,-1}, \boldsymbol{x}_{i} \right)$ and $\boldsymbol{\varphi}=\left( \phi, \beta \right)^{'}$


\subsection{LSMG estimator}
The LS (least square) estimator is defined as
\begin{align}
\boldsymbol{\hat{\varphi}}_{LSi}=\left(\boldsymbol{W}^{'}_{i}\boldsymbol{W}_{i}  \right)^{-1}\left(\boldsymbol{W}^{'}_{i}\boldsymbol{y}_{i}  \right)
\end{align}
Follow \citet{Pesaran:1995}, we define the LSMG (least square mean group) estimator as
\begin{align}
\boldsymbol{\hat{\varphi}}_{LSMG}=\frac{1}{N}\sum^{N}_{i=1}\boldsymbol{\hat{\varphi}}_{LSi}.
\end{align}

\subsection{IVMG estimator}
We use current and lagged values of $\boldsymbol{x}_{i}$ as instruments, as
\begin{align}
\boldsymbol{Z}_{i}=\left( \boldsymbol{x}_{i}, \boldsymbol{x}_{i,-1} \right),
\end{align}
where $\boldsymbol{x}_{i,-1} =\left(x_{i,0},  \ldots, x_{i,T-1} \right)^{'}$.

The IV (instrument variable) estimator is defined as
\begin{align}
\boldsymbol{\hat{\varphi}}_{IVi}=\left(\boldsymbol{Z}^{'}_{i}\boldsymbol{W}_{i}  \right)^{-1}\left(\boldsymbol{Z}^{'}_{i}\boldsymbol{y}_{i}  \right)
\end{align}
 We also define the IVMG (instrument variable mean group) estimator as
\begin{align}
\boldsymbol{\hat{\varphi}}_{IVMG}=\frac{1}{N}\sum^{N}_{i=1}\boldsymbol{\hat{\varphi}}_{IVi}.
\end{align}



\subsection{Norkutes' (2019) IVMG estimator}
Consider the dynamic heterogeneous panel data model with multi-factor error structure. The error term $u_{i,t}$ and $\xi_{i,t}$ can be extend as
\begin{align}
\begin{split}
u_{i,t}&=\boldsymbol{\gamma}^{0'}_{yi}\boldsymbol{f}^{0}_{yt}+\varepsilon_{yi,t}, \\
\xi_{i,t}&=\boldsymbol{\gamma}^{0'}_{xi}\boldsymbol{f}^{0}_{xt}+\varepsilon_{xi,t},
\end{split}
\end{align}
where $\boldsymbol{\gamma}^{0}_{yi}$ and $\boldsymbol{\gamma}^{0}_{xi}$ are $m_{y}\times 1$ and $m_{x}\times 1$ true
 factor loading respectively, $\boldsymbol{f}^{0}_{yt}$  and  $\boldsymbol{f}^{0}_{xt}$ are  $m_{y}\times 1$ and $m_{x}\times 1$ true vector of unobservable factors respectively. And, we set  $\varepsilon_{yi,t}\sim \mathcal{N}(0,\,\sigma_{u}^{2})\,,$ , $\varepsilon_{xi,t}\sim \mathcal{N}(0,\,\sigma_{\xi}^{2})\,.$
 In the first step, we asymptotically eliminate the common factor in $\boldsymbol{x}_{i}$ by projecting matrix, $\boldsymbol{M}_{F^{0}_{x}}$.
 \begin{align}
 \boldsymbol{M}_{F^{0}_{x}}=\boldsymbol{I}_{T}-\boldsymbol{F}^{0}_{x}\left(\boldsymbol{F}^{0'}_{x}\boldsymbol{F}^{0}_{x}  \right)^{-1}\boldsymbol{F}^{0'}_{x} ; \boldsymbol{M}_{F^{0}_{x,-1}}=\boldsymbol{I}_{T}-\boldsymbol{F}^{0}_{x,-1}\left(\boldsymbol{F}^{0'}_{x,-1}\boldsymbol{F}^{0}_{x,-1}  \right)^{-1}\boldsymbol{F}^{0'}_{x,-1}
 \end{align}
And using the defactored covariates as instruments, as
\begin{align}
\boldsymbol{Z}_{IVi}=\left(\boldsymbol{M}_{F^{0}_{x}}\boldsymbol{x}_{i}, \boldsymbol{M}_{F^{0}_{x,-1}}\boldsymbol{x}_{i,-1}  \right)
\end{align}
We use $\boldsymbol{Z}_{IVi}$ as instruments to obtain first step estimator. Then, we can use CCE (common correlated effects) method or PC (principal component) methods to estimate $\boldsymbol{F}^{0}_{y}$ from the residuals in first step IV regression. In second step, we asymptotically eliminate the common factor, $\boldsymbol{F}^{0}_{y}$ by projecting matrix, $\boldsymbol{M}_{F^{0}_{y}}$.
Therefore, we premultiply $\boldsymbol{M}_{F^{0}_{y}}$ in both side of equation $(\ref{7})$ as
\begin{align}
\boldsymbol{M}_{F^{0}_{y}}\boldsymbol{y}_{i}=\boldsymbol{M}_{F^{0}_{y}} \boldsymbol{W}_{i}\boldsymbol{\varphi}+\boldsymbol{M}_{F^{0}_{y}}\boldsymbol{\varepsilon}_{i},
\end{align}
 where $\boldsymbol{\varepsilon}_{i}=\left(\varepsilon_{i,1}, \ldots, \varepsilon_{i,T} \right)$.
The second step IV estimator can be expressed as
\begin{align}
\begin{split}
\boldsymbol{\hat{\hat{\varphi}}}_{IV}&=\left(    \left(\boldsymbol{W}^{'}_{i}\boldsymbol{M}_{F^{0}_{y}}\boldsymbol{Z}_{IVi} \right) \left( \boldsymbol{Z}^{'}_{IVi} \boldsymbol{M}_{F^{0}_{y}}\boldsymbol{Z}_{IVi}  \right)^{-1}    \left(  \boldsymbol{Z}^{'}_{IVi}  \boldsymbol{M}_{F^{0}_{y}} \boldsymbol{W} \right)\right)^{-1} \times \\
& \left(\boldsymbol{W}^{'}_{i}\boldsymbol{M}_{F^{0}_{y}}\boldsymbol{Z}_{IVi} \right) \left( \boldsymbol{Z}^{'}_{IVi} \boldsymbol{M}_{F^{0}_{y}}\boldsymbol{Z}_{IVi}  \right)^{-1}  \left(  \boldsymbol{Z}^{'}_{IVi}  \boldsymbol{M}_{F^{0}_{y}} \boldsymbol{y}_{i} \right)
\end{split}
\end{align}








\section{MC setting}
\subsection{Dynamic heterogeneous panels data model without multi-factor error structure }
Consider the dynamic heterogeneous panels data model:
\begin{align}
\begin{split}
y_{i,t}&=\phi_{i} y_{i,t-1}+ \beta_{1i}x_{i,t}+u_{i,t}, \,\, for\,\,i=1,\ldots N;\,t=1,\ldots,T\, , \\ \label{M1}
x_{i,t}&=\rho x_{i,t-1}+\xi_{i,t},
\end{split}
\end{align}
where $u_{i,t}\sim \mathcal{N}(0,\,\sigma_{u}^{2})\,,$ and $\xi_{i,t}\sim \mathcal{N}(0,\,\sigma_{\xi}^{2})\,.$

\subsection{Dynamic heterogeneous panels data model with multi-factor error structure }
In dynamic panel data 
The generating process of $y$ and $x$ follow $(\ref{M1})$, but we assume
\begin{align}
\begin{split}
u_{i,t}&=\boldsymbol{\gamma}^{'}_{yi}\boldsymbol{f}_{yt}+\varepsilon_{yi,t}, \\
\xi_{i,t}&=\boldsymbol{\gamma}^{'}_{xi}\boldsymbol{f}_{xt}+\varepsilon_{xi,t},
\end{split}
\end{align}
where $\boldsymbol{\gamma}_{yi}$ and $\boldsymbol{\gamma}_{xi}$ are $m_{y}\times 1$ and $m_{x}\times 1$
 factor loading respectively, $\boldsymbol{f}_{yt}$  and  $\boldsymbol{f}_{xt}$ are  $m_{y}\times 1$ and $m_{x}\times 1$ unobservable factors respectively. And, we set  $\varepsilon_{yi,t}\sim \mathcal{N}(0,\,\sigma_{u}^{2})\,,$ , $\varepsilon_{xi,t}\sim \mathcal{N}(0,\,\sigma_{\xi}^{2})\,.$

\begin{align}
\begin{split}
\phi_{i}&= \phi+\eta_{1i}    \\
\beta_{1i}&= \beta_{1}+\eta_{2i},
\end{split}
\end{align}
where $\eta_{1i}\sim \mathcal{IID}\,\mathcal{U}(0.5,\, 1)\,,$ and $\eta_{2i}\sim \mathcal{IID}\,\mathcal{U}(0,\, 0.8)\,$
We also try $\phi_{i}$ is fixed across group, which means $\phi_{i}= \phi$.

We chose $\phi=\{0.25 \}$ and $\beta_{1}=1-\phi$ and $\rho=\{0, 0.5 \}$. Also, we selected $\sigma_{u}^{2}=1$ and $\sigma_{\xi}^{2}=1$. The replication is 1000.
We define LSMG (lest square mean group) estimator and instrument variable mean group (IVMG) estimator.


In the simulation results, we provide the bias and RMSE (root mean square errors) for LSMG estimator and IVMG estimator.






\section{MC results}
Report in excel file. \\
In Excel file:\\
\subsection{Model  without factor structure }
Sheet 1: ARDL(1,0); $\phi_{i}$ ; $\beta_{i}$ .\\
Sheet 2: ARDL(1,0); $\phi$ ; $\beta_{i}$ .\\
Sheet 3: ARDL(1,1); $\phi_{i}$ ; $\beta_{i}$ .\\
Sheet 4: ARDL(1,1); $\phi$ ; $\beta_{i}$  . \\

\subsection{Model have factor structure 1}
We estimate the number of factor is $0$, so we use traditional OLS estimation method and IV estimation method.  \\
Sheet 1: ARDL(1,0); $\phi_{i}$ ; $\beta_{i}$. \\
Sheet 2: ARDL(1,0); $\phi$ ; $\beta_{i}$ \\
Sheet 3: ARDL(1,1); $\phi_{i}$ ; $\beta_{i}$. \\
Sheet 4: ARDL(1,1); $\phi$ ; $\beta_{i}$  . \\

\subsection{Model have factor structure 2}
 We provide MC simulation results for IV estimator that is been provided by \citet{Norkute:2019}. \\
 Sheet 1: ARDL(1,0); $\phi_{i}$ ; $\beta_{i}$. \\
Sheet 2: ARDL(1,0); $\phi$ ; $\beta_{i}$ .\\
Sheet 3: ARDL(1,1); $\phi_{i}$ ; $\beta_{i}$ .\\
Sheet 4: ARDL(1,1); $\phi$ ; $\beta_{i}$.   \\















\newpage

\addcontentsline{toc}{section}{Reference}
\renewcommand\refname{References}
\bibliographystyle{chicago}
\bibliography{1}

\end{document}  \href{*}{*} 